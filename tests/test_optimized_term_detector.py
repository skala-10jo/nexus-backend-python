"""
OptimizedTermDetectorAgent í…ŒìŠ¤íŠ¸

Aho-Corasick ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ìš©ì–´ íƒì§€ ì„±ëŠ¥ ë° ì •í™•ë„ í…ŒìŠ¤íŠ¸
"""

import pytest
import asyncio
import time
from typing import List, Dict

# Agent imports
from agent.term_detection.optimized_term_detector_agent import (
    OptimizedTermDetectorAgent,
    DetectedTerm,
    AutomatonCache
)
from agent.term_detection.term_detector_agent import (
    TermDetectorAgent,
    DetectedTerm as LegacyDetectedTerm
)


# í…ŒìŠ¤íŠ¸ìš© ìš©ì–´ì§‘
SAMPLE_GLOSSARY = [
    {"korean_term": "ì¸ê³µì§€ëŠ¥", "english_term": "Artificial Intelligence", "vietnamese_term": "TrÃ­ tuá»‡ nhÃ¢n táº¡o"},
    {"korean_term": "ë¨¸ì‹ ëŸ¬ë‹", "english_term": "Machine Learning", "vietnamese_term": "Há»c mÃ¡y"},
    {"korean_term": "ë”¥ëŸ¬ë‹", "english_term": "Deep Learning", "vietnamese_term": "Há»c sÃ¢u"},
    {"korean_term": "í´ë¼ìš°ë“œ", "english_term": "Cloud", "vietnamese_term": "ÄÃ¡m mÃ¢y"},
    {"korean_term": "ì»¨í…Œì´ë„ˆ", "english_term": "Container", "vietnamese_term": "Container"},
    {"korean_term": "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤", "english_term": "Microservices", "vietnamese_term": "Vi dá»‹ch vá»¥"},
    {"korean_term": "ì¿ ë²„ë„¤í‹°ìŠ¤", "english_term": "Kubernetes", "vietnamese_term": "Kubernetes"},
    {"korean_term": "ë„ì»¤", "english_term": "Docker", "vietnamese_term": "Docker"},
    {"korean_term": "API", "english_term": "API", "vietnamese_term": "API"},
    {"korean_term": "REST", "english_term": "REST", "vietnamese_term": "REST"},
]


class TestOptimizedTermDetectorAgent:
    """OptimizedTermDetectorAgent í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    @pytest.fixture
    def agent(self):
        """í…ŒìŠ¤íŠ¸ìš© Agent ì¸ìŠ¤í„´ìŠ¤"""
        return OptimizedTermDetectorAgent()

    @pytest.fixture
    def legacy_agent(self):
        """ê¸°ì¡´ Agent ì¸ìŠ¤í„´ìŠ¤ (ë¹„êµìš©)"""
        return TermDetectorAgent()

    @pytest.mark.asyncio
    async def test_basic_detection(self, agent):
        """ê¸°ë³¸ ìš©ì–´ íƒì§€ í…ŒìŠ¤íŠ¸"""
        text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤"

        detected = await agent.process(text, SAMPLE_GLOSSARY, source_lang="ko")

        assert len(detected) == 2
        terms = [d.korean_term for d in detected]
        assert "ì¸ê³µì§€ëŠ¥" in terms
        assert "ë¨¸ì‹ ëŸ¬ë‹" in terms

    @pytest.mark.asyncio
    async def test_position_accuracy(self, agent):
        """ìœ„ì¹˜ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        text = "í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì»¨í…Œì´ë„ˆë¥¼ ë°°í¬í•©ë‹ˆë‹¤"

        detected = await agent.process(text, SAMPLE_GLOSSARY, source_lang="ko")

        # í´ë¼ìš°ë“œ: 0~4
        cloud_term = next(d for d in detected if d.korean_term == "í´ë¼ìš°ë“œ")
        assert cloud_term.position_start == 0
        assert cloud_term.position_end == 4
        assert cloud_term.matched_text == "í´ë¼ìš°ë“œ"

        # ì»¨í…Œì´ë„ˆ: 10~14
        container_term = next(d for d in detected if d.korean_term == "ì»¨í…Œì´ë„ˆ")
        assert container_term.position_start == 10
        assert container_term.position_end == 14
        assert container_term.matched_text == "ì»¨í…Œì´ë„ˆ"

    @pytest.mark.asyncio
    async def test_no_duplicate_matching(self, agent):
        """ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€ í…ŒìŠ¤íŠ¸"""
        # "ì¸ê³µì§€ëŠ¥"ì´ í¬í•¨ëœ ë” ê¸´ ìš©ì–´ê°€ ìˆëŠ” ê²½ìš°
        glossary = [
            {"korean_term": "ì¸ê³µì§€ëŠ¥", "english_term": "AI"},
            {"korean_term": "ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œ", "english_term": "AI System"},
        ]
        text = "ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤"

        detected = await agent.process(text, glossary, source_lang="ko")

        # ê¸´ ìš©ì–´ ìš°ì„  ë§¤ì¹­, ì¤‘ë³µ ì—†ì–´ì•¼ í•¨
        assert len(detected) == 1
        assert detected[0].korean_term == "ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œ"

    @pytest.mark.asyncio
    async def test_korean_word_boundary(self, agent):
        """í•œê¸€ ë‹¨ì–´ ê²½ê³„ í…ŒìŠ¤íŠ¸"""
        glossary = [{"korean_term": "ì‹œìŠ¤í…œ", "english_term": "System"}]

        # "ì‹œìŠ¤í…œ"ì´ ë…ë¦½ì ìœ¼ë¡œ ë“±ì¥í•´ì•¼ ë§¤ì¹­
        text1 = "ì‹œìŠ¤í…œì„ ê°œë°œí•©ë‹ˆë‹¤"  # ë§¤ì¹­ O
        text2 = "ìƒíƒœì‹œìŠ¤í…œ êµ¬ì¶•"      # ë§¤ì¹­ X (ì•ì— í•œê¸€)

        detected1 = await agent.process(text1, glossary, source_lang="ko")
        detected2 = await agent.process(text2, glossary, source_lang="ko")

        assert len(detected1) == 1
        assert len(detected2) == 0  # ì•ì— í•œê¸€ì´ ë¶™ì–´ìˆìœ¼ë¯€ë¡œ ë§¤ì¹­ ì•ˆë¨

    @pytest.mark.asyncio
    async def test_english_detection(self, agent):
        """ì˜ì–´ ìš©ì–´ íƒì§€ í…ŒìŠ¤íŠ¸"""
        text = "We use Machine Learning and Deep Learning"

        detected = await agent.process(text, SAMPLE_GLOSSARY, source_lang="en")

        assert len(detected) == 2
        terms = [d.english_term for d in detected]
        assert "Machine Learning" in terms
        assert "Deep Learning" in terms

    @pytest.mark.asyncio
    async def test_case_insensitive(self, agent):
        """ëŒ€ì†Œë¬¸ì ë¬´ì‹œ í…ŒìŠ¤íŠ¸"""
        text = "api and rest are important"

        detected = await agent.process(text, SAMPLE_GLOSSARY, source_lang="en")

        assert len(detected) == 2

    @pytest.mark.asyncio
    async def test_empty_input(self, agent):
        """ë¹ˆ ì…ë ¥ í…ŒìŠ¤íŠ¸"""
        with pytest.raises(ValueError):
            await agent.process("", SAMPLE_GLOSSARY)

        with pytest.raises(ValueError):
            await agent.process("   ", SAMPLE_GLOSSARY)

    @pytest.mark.asyncio
    async def test_empty_glossary(self, agent):
        """ë¹ˆ ìš©ì–´ì§‘ í…ŒìŠ¤íŠ¸"""
        text = "ì¸ê³µì§€ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤"

        detected = await agent.process(text, [], source_lang="ko")

        assert len(detected) == 0

    @pytest.mark.asyncio
    async def test_cache_functionality(self, agent):
        """ìºì‹œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹"

        # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
        detected1 = await agent.process(text, SAMPLE_GLOSSARY, source_lang="ko")

        # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸ ì˜ˆìƒ)
        detected2 = await agent.process(text, SAMPLE_GLOSSARY, source_lang="ko")

        # ê²°ê³¼ê°€ ë™ì¼í•´ì•¼ í•¨
        assert len(detected1) == len(detected2)
        for d1, d2 in zip(detected1, detected2):
            assert d1.korean_term == d2.korean_term
            assert d1.position_start == d2.position_start

    @pytest.mark.asyncio
    async def test_performance_vs_legacy(self, agent, legacy_agent):
        """ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ (Optimized vs Legacy)"""
        # ëŒ€ê·œëª¨ ìš©ì–´ì§‘ ìƒì„±
        large_glossary = [
            {"korean_term": f"ìš©ì–´{i}", "english_term": f"Term{i}"}
            for i in range(1000)
        ]
        large_glossary.extend(SAMPLE_GLOSSARY)

        # ê¸´ í…ìŠ¤íŠ¸ ìƒì„±
        text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ í´ë¼ìš°ë“œ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. " * 50

        # Optimized Agent ì„±ëŠ¥ ì¸¡ì •
        start_optimized = time.time()
        detected_optimized = await agent.process(text, large_glossary, source_lang="ko")
        time_optimized = time.time() - start_optimized

        # Legacy Agent ì„±ëŠ¥ ì¸¡ì •
        start_legacy = time.time()
        detected_legacy = await legacy_agent.process(text, large_glossary, source_lang="ko")
        time_legacy = time.time() - start_legacy

        print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ (1,000 ìš©ì–´ Ã— {len(text)}ì):")
        print(f"  - Optimized (Aho-Corasick): {time_optimized:.4f}ì´ˆ")
        print(f"  - Legacy (Regex): {time_legacy:.4f}ì´ˆ")
        print(f"  - ì„±ëŠ¥ í–¥ìƒ: {time_legacy / time_optimized:.1f}x")

        # ê²°ê³¼ëŠ” ë™ì¼í•´ì•¼ í•¨
        assert len(detected_optimized) == len(detected_legacy)

        # Optimizedê°€ ë” ë¹¨ë¼ì•¼ í•¨ (ìµœì†Œ 2ë°° ì´ìƒ)
        # ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” ì˜¤ë²„í—¤ë“œë¡œ ì¸í•´ ì°¨ì´ê°€ ì ì„ ìˆ˜ ìˆìŒ
        # assert time_optimized < time_legacy

    @pytest.mark.asyncio
    async def test_result_compatibility(self, agent, legacy_agent):
        """ê²°ê³¼ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ (Legacyì™€ ë™ì¼í•œ ê²°ê³¼ ë³´ì¥)"""
        text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹, í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì»¨í…Œì´ë„ˆë¥¼ ë°°í¬í•©ë‹ˆë‹¤"

        detected_optimized = await agent.process(text, SAMPLE_GLOSSARY, source_lang="ko")
        detected_legacy = await legacy_agent.process(text, SAMPLE_GLOSSARY, source_lang="ko")

        # íƒì§€ëœ ìš©ì–´ ìˆ˜ ë™ì¼
        assert len(detected_optimized) == len(detected_legacy)

        # ê° ìš©ì–´ ì •ë³´ ë™ì¼
        for opt, leg in zip(detected_optimized, detected_legacy):
            assert opt.korean_term == leg.korean_term
            assert opt.english_term == leg.english_term
            assert opt.position_start == leg.position_start
            assert opt.position_end == leg.position_end
            assert opt.matched_text == leg.matched_text


class TestAutomatonCache:
    """AutomatonCache í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_cache_basic(self):
        """ê¸°ë³¸ ìºì‹œ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        cache = AutomatonCache(max_size=10)

        # ì´ˆê¸° ìƒíƒœ: ìºì‹œ ë¹„ì–´ìˆìŒ
        assert cache.get(SAMPLE_GLOSSARY, "korean_term") is None

        # Automaton ìƒì„± ë° ì €ì¥ (ìˆ˜ë™ í…ŒìŠ¤íŠ¸ìš©)
        # ì‹¤ì œë¡œëŠ” Agentê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬

    def test_cache_clear(self):
        """ìºì‹œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        cache = AutomatonCache()
        cache.clear()
        # ì˜ˆì™¸ ì—†ì´ ì™„ë£Œë˜ì–´ì•¼ í•¨

    def test_cache_normalize_mode_separation(self):
        """ì •ê·œí™” ëª¨ë“œë³„ ìºì‹œ ë¶„ë¦¬ í…ŒìŠ¤íŠ¸"""
        cache = AutomatonCache(max_size=10)

        # ê°™ì€ ìš©ì–´ì§‘ì´ë¼ë„ ì •ê·œí™” ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ìºì‹œ í‚¤ ìƒì„±
        key_normal = cache._generate_cache_key(SAMPLE_GLOSSARY, "korean_term", normalize_mode=False)
        key_normalized = cache._generate_cache_key(SAMPLE_GLOSSARY, "korean_term", normalize_mode=True)

        # ë‘ í‚¤ê°€ ë‹¬ë¼ì•¼ í•¨
        assert key_normal != key_normalized
        assert "_norm" not in key_normal
        assert "_norm" in key_normalized


class TestWhitespaceNormalization:
    """ë„ì–´ì“°ê¸° ì •ê·œí™” í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    @pytest.fixture
    def agent(self):
        """í…ŒìŠ¤íŠ¸ìš© Agent ì¸ìŠ¤í„´ìŠ¤"""
        return OptimizedTermDetectorAgent()

    @pytest.mark.asyncio
    async def test_glossary_with_space_text_without_space(self, agent):
        """ìš©ì–´ì§‘ì— ê³µë°± ìˆìŒ, í…ìŠ¤íŠ¸ì— ê³µë°± ì—†ìŒ"""
        glossary = [
            {"korean_term": "ì¸ê³µ ì§€ëŠ¥", "english_term": "Artificial Intelligence"},
            {"korean_term": "ë¨¸ì‹  ëŸ¬ë‹", "english_term": "Machine Learning"},
        ]

        text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•©ë‹ˆë‹¤"

        detected = await agent.process(text, glossary, source_lang="ko")

        assert len(detected) == 2

        # ì²« ë²ˆì§¸ ìš©ì–´: ì¸ê³µì§€ëŠ¥
        assert detected[0].matched_text == "ì¸ê³µì§€ëŠ¥"
        assert detected[0].korean_term == "ì¸ê³µ ì§€ëŠ¥"
        assert detected[0].position_start == 0
        assert detected[0].position_end == 4

        # ë‘ ë²ˆì§¸ ìš©ì–´: ë¨¸ì‹ ëŸ¬ë‹
        assert detected[1].matched_text == "ë¨¸ì‹ ëŸ¬ë‹"
        assert detected[1].korean_term == "ë¨¸ì‹  ëŸ¬ë‹"

    @pytest.mark.asyncio
    async def test_glossary_without_space_text_with_space(self, agent):
        """ìš©ì–´ì§‘ì— ê³µë°± ì—†ìŒ, í…ìŠ¤íŠ¸ì— ê³µë°± ìˆìŒ"""
        glossary = [
            {"korean_term": "ë”¥ëŸ¬ë‹", "english_term": "Deep Learning"},
            {"korean_term": "í´ë¼ìš°ë“œ", "english_term": "Cloud"},
        ]

        text = "ë”¥ ëŸ¬ë‹ê³¼ í´ë¼ ìš°ë“œ ê¸°ìˆ ì„ ë°°ì›ë‹ˆë‹¤"

        detected = await agent.process(text, glossary, source_lang="ko")

        assert len(detected) == 2

        # ì²« ë²ˆì§¸ ìš©ì–´: ë”¥ ëŸ¬ë‹ (ê³µë°± í¬í•¨)
        assert detected[0].matched_text == "ë”¥ ëŸ¬ë‹"
        assert detected[0].korean_term == "ë”¥ëŸ¬ë‹"
        assert detected[0].position_start == 0
        assert detected[0].position_end == 4

        # ë‘ ë²ˆì§¸ ìš©ì–´: í´ë¼ ìš°ë“œ (ê³µë°± í¬í•¨)
        assert detected[1].matched_text == "í´ë¼ ìš°ë“œ"
        assert detected[1].korean_term == "í´ë¼ìš°ë“œ"

    @pytest.mark.asyncio
    async def test_mixed_whitespace_variations(self, agent):
        """ë‹¤ì–‘í•œ ë„ì–´ì“°ê¸° ë³€í˜• í˜¼í•© í…ŒìŠ¤íŠ¸"""
        glossary = [
            {"korean_term": "ìì—°ì–´ ì²˜ë¦¬", "english_term": "NLP"},
        ]

        # ë‹¤ì–‘í•œ ë„ì–´ì“°ê¸° ë³€í˜• í…ŒìŠ¤íŠ¸
        test_cases = [
            ("ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤", "ìì—°ì–´ì²˜ë¦¬"),       # ê³µë°± ì—†ìŒ
            ("ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤", "ìì—°ì–´ ì²˜ë¦¬"),     # ê³µë°± ìˆìŒ (ì›ë³¸)
            ("ì ì—° ì–´ ì²˜ ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤", "ì ì—° ì–´ ì²˜ ë¦¬"),  # ê³µë°± ë§ìŒ
        ]

        for text, expected_match in test_cases:
            detected = await agent.process(text, glossary, source_lang="ko")
            assert len(detected) == 1, f"Failed for: {text}"
            assert detected[0].matched_text == expected_match, f"Failed for: {text}"
            assert detected[0].korean_term == "ìì—°ì–´ ì²˜ë¦¬"

    @pytest.mark.asyncio
    async def test_position_accuracy_with_normalization(self, agent):
        """ì •ê·œí™” ì‹œ ìœ„ì¹˜ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        glossary = [{"korean_term": "ë°ì´í„° ë¶„ì„", "english_term": "Data Analysis"}]

        text = "ìš”ì¦˜ ë°ì´í„°ë¶„ì„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤"
        #       01234 567890123456789

        detected = await agent.process(text, glossary, source_lang="ko")

        assert len(detected) == 1
        assert detected[0].matched_text == "ë°ì´í„°ë¶„ì„"
        assert detected[0].position_start == 3
        assert detected[0].position_end == 8
        # text[3:8] == "ë°ì´í„°ë¶„ì„" í™•ì¸
        assert text[detected[0].position_start:detected[0].position_end] == "ë°ì´í„°ë¶„ì„"

    @pytest.mark.asyncio
    async def test_normalization_disabled(self, agent):
        """ì •ê·œí™” ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸"""
        glossary = [{"korean_term": "ì¸ê³µ ì§€ëŠ¥", "english_term": "AI"}]

        text = "ì¸ê³µì§€ëŠ¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤"

        # ì •ê·œí™” ë¹„í™œì„±í™” ì‹œ ë§¤ì¹­ ì•ˆë¨
        detected = await agent.process(
            text, glossary, source_lang="ko",
            normalize_whitespace=False
        )
        assert len(detected) == 0

        # ì •ê·œí™” í™œì„±í™” ì‹œ ë§¤ì¹­ë¨
        detected = await agent.process(
            text, glossary, source_lang="ko",
            normalize_whitespace=True
        )
        assert len(detected) == 1

    @pytest.mark.asyncio
    async def test_english_no_normalization_by_default(self, agent):
        """ì˜ì–´ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì •ê·œí™” ë¹„ì ìš© í…ŒìŠ¤íŠ¸"""
        glossary = [
            {"korean_term": "ë‰´ìš•", "english_term": "New York"},
        ]

        # ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ê³µë°± ì—†ì´ ê²€ìƒ‰
        text = "I visited NewYork last year"

        # ì˜ì–´ëŠ” NORMALIZE_LANGUAGESì— ì—†ìœ¼ë¯€ë¡œ ì •ê·œí™” ë¹„ì ìš©
        detected = await agent.process(text, glossary, source_lang="en")

        # "NewYork"ì€ "New York"ê³¼ ë‹¤ë¥´ë¯€ë¡œ ë§¤ì¹­ ì•ˆë¨
        assert len(detected) == 0

        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        text2 = "I visited New York last year"
        detected2 = await agent.process(text2, glossary, source_lang="en")
        assert len(detected2) == 1
        assert detected2[0].matched_text == "New York"

    @pytest.mark.asyncio
    async def test_no_duplicate_with_normalization(self, agent):
        """ì •ê·œí™” ëª¨ë“œì—ì„œ ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€ í…ŒìŠ¤íŠ¸"""
        glossary = [
            {"korean_term": "ê¸°ê³„ í•™ìŠµ", "english_term": "ML"},
            {"korean_term": "ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œ", "english_term": "ML System"},
        ]

        text = "ê¸°ê³„í•™ìŠµì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤"

        detected = await agent.process(text, glossary, source_lang="ko")

        # ê¸´ ìš©ì–´ ìš°ì„ , ì¤‘ë³µ ì—†ì–´ì•¼ í•¨
        assert len(detected) == 1
        assert detected[0].korean_term == "ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œ"
        assert detected[0].matched_text == "ê¸°ê³„í•™ìŠµì‹œìŠ¤í…œ"

    @pytest.mark.asyncio
    async def test_multiple_occurrences_with_normalization(self, agent):
        """ì •ê·œí™” ëª¨ë“œì—ì„œ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ëŠ” ìš©ì–´ í…ŒìŠ¤íŠ¸"""
        glossary = [{"korean_term": "ì¸ê³µ ì§€ëŠ¥", "english_term": "AI"}]

        text = "ì¸ê³µì§€ëŠ¥ì˜ ë°œì „, ì¸ê³µ ì§€ëŠ¥ì˜ ë¯¸ë˜, ì¸ê³µ  ì§€ëŠ¥ì˜ ì‘ìš©"

        detected = await agent.process(text, glossary, source_lang="ko")

        # ì„¸ ë²ˆ ëª¨ë‘ íƒì§€ë˜ì–´ì•¼ í•¨
        assert len(detected) == 3

        # ê°ê°ì˜ ë§¤ì¹­ í…ìŠ¤íŠ¸ í™•ì¸
        matched_texts = [d.matched_text for d in detected]
        assert "ì¸ê³µì§€ëŠ¥" in matched_texts
        assert "ì¸ê³µ ì§€ëŠ¥" in matched_texts
        assert "ì¸ê³µ  ì§€ëŠ¥" in matched_texts

    @pytest.mark.asyncio
    async def test_korean_word_boundary_with_normalization(self, agent):
        """ì •ê·œí™” ëª¨ë“œì—ì„œ í•œê¸€ ë‹¨ì–´ ê²½ê³„ í…ŒìŠ¤íŠ¸"""
        glossary = [{"korean_term": "ì‹œìŠ¤í…œ", "english_term": "System"}]

        # "ì‹œìŠ¤í…œ"ì´ ë…ë¦½ì ìœ¼ë¡œ ë“±ì¥í•´ì•¼ ë§¤ì¹­
        text1 = "ì‹œ ìŠ¤ í…œì„ ê°œë°œí•©ë‹ˆë‹¤"  # ë§¤ì¹­ O (ê³µë°± ì •ê·œí™”)
        text2 = "ìƒíƒœì‹œìŠ¤í…œ êµ¬ì¶•"         # ë§¤ì¹­ X (ì•ì— í•œê¸€)

        detected1 = await agent.process(text1, glossary, source_lang="ko")
        detected2 = await agent.process(text2, glossary, source_lang="ko")

        assert len(detected1) == 1
        assert detected1[0].matched_text == "ì‹œ ìŠ¤ í…œ"
        assert len(detected2) == 0  # ì•ì— í•œê¸€ì´ ë¶™ì–´ìˆìœ¼ë¯€ë¡œ ë§¤ì¹­ ì•ˆë¨


class TestPositionMapping:
    """PositionMapping í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    @pytest.fixture
    def agent(self):
        return OptimizedTermDetectorAgent()

    def test_create_position_mapping(self, agent):
        """ìœ„ì¹˜ ë§¤í•‘ ìƒì„± í…ŒìŠ¤íŠ¸"""
        text = "ì¸ê³µ ì§€ëŠ¥ê³¼ ë¨¸ì‹  ëŸ¬ë‹"

        mapping = agent._create_position_mapping(text)

        assert mapping.original_text == text
        assert mapping.normalized_text == "ì¸ê³µì§€ëŠ¥ê³¼ë¨¸ì‹ ëŸ¬ë‹"
        assert len(mapping.norm_to_orig) == len(mapping.normalized_text)

        # ë§¤í•‘ ê²€ì¦
        # ì›ë³¸:    ì¸ ê³µ   ì§€ ëŠ¥ ê³¼ ë¨¸ ì‹    ëŸ¬ ë‹
        # ì¸ë±ìŠ¤:  0  1  2  3  4  5  6  7  8  9  10
        # ì •ê·œí™”:  ì¸ ê³µ ì§€ ëŠ¥ ê³¼ ë¨¸ ì‹  ëŸ¬ ë‹
        # ì¸ë±ìŠ¤:  0  1  2  3  4  5  6  7  8
        expected_mapping = [0, 1, 3, 4, 5, 7, 8, 10, 11]
        assert mapping.norm_to_orig == expected_mapping

    def test_map_to_original_position(self, agent):
        """ìœ„ì¹˜ ì—­ì‚° í…ŒìŠ¤íŠ¸"""
        text = "ì¸ê³µ ì§€ëŠ¥ê³¼ ë¨¸ì‹  ëŸ¬ë‹"
        mapping = agent._create_position_mapping(text)

        # ì •ê·œí™”ëœ "ì¸ê³µì§€ëŠ¥" (0~4)ì„ ì›ë³¸ìœ¼ë¡œ ì—­ì‚°
        orig_start, orig_end = agent._map_to_original_position(0, 4, mapping)

        assert orig_start == 0
        assert orig_end == 5  # "ì¸ê³µ ì§€" ê¹Œì§€ (ê³µë°± í¬í•¨)
        assert text[orig_start:orig_end] == "ì¸ê³µ ì§€ëŠ¥"

    def test_normalize_term(self, agent):
        """ìš©ì–´ ì •ê·œí™” í…ŒìŠ¤íŠ¸"""
        assert agent._normalize_term("ì¸ê³µ ì§€ëŠ¥") == "ì¸ê³µì§€ëŠ¥"
        assert agent._normalize_term("ë¨¸ì‹   ëŸ¬ë‹") == "ë¨¸ì‹ ëŸ¬ë‹"
        assert agent._normalize_term("  ë”¥ ëŸ¬ë‹  ") == "ë”¥ëŸ¬ë‹"
        assert agent._normalize_term("NoSpace") == "NoSpace"


# CLIì—ì„œ ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬
async def run_benchmark():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸš€ OptimizedTermDetectorAgent ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)

    agent = OptimizedTermDetectorAgent()
    legacy_agent = TermDetectorAgent()

    # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ìš©ì–´ì§‘ í…ŒìŠ¤íŠ¸
    for num_terms in [100, 500, 1000, 5000]:
        glossary = [
            {"korean_term": f"ìš©ì–´{i}", "english_term": f"Term{i}"}
            for i in range(num_terms)
        ]
        glossary.extend(SAMPLE_GLOSSARY)

        # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        for text_multiplier in [1, 10, 50]:
            base_text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ í´ë¼ìš°ë“œ ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. "
            text = base_text * text_multiplier

            # Warmup (ìºì‹œ ë¹Œë“œ)
            await agent.process(text, glossary, source_lang="ko")

            # ì„±ëŠ¥ ì¸¡ì • (ìºì‹œ í™œìš©)
            start = time.time()
            for _ in range(10):
                await agent.process(text, glossary, source_lang="ko")
            time_optimized = (time.time() - start) / 10

            # Legacy ì¸¡ì •
            start = time.time()
            for _ in range(10):
                await legacy_agent.process(text, glossary, source_lang="ko")
            time_legacy = (time.time() - start) / 10

            speedup = time_legacy / time_optimized if time_optimized > 0 else 0

            print(f"\nğŸ“Š {num_terms} ìš©ì–´ Ã— {len(text)}ì:")
            print(f"   Optimized: {time_optimized*1000:.2f}ms")
            print(f"   Legacy:    {time_legacy*1000:.2f}ms")
            print(f"   Speedup:   {speedup:.1f}x")

    # ìºì‹œ ì´ˆê¸°í™”
    OptimizedTermDetectorAgent.clear_cache()
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")


if __name__ == "__main__":
    asyncio.run(run_benchmark())
