"""
Azure Speech SDKë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ ë° í™”ì ë¶„ë¦¬ ì—ì´ì „íŠ¸.

ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µ:
- í™”ìë³„ë¡œ ë¶„ë¦¬ëœ ë°œí™” ë‚´ìš©
- íƒ€ì„ìŠ¤íƒ¬í”„ (ì‹œì‘/ì¢…ë£Œ ì‹œê°„)
- ì‹ ë¢°ë„ ì ìˆ˜

Docker/AWS í™˜ê²½ í˜¸í™˜:
- PushAudioInputStreamì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
- AudioConfig(filename=...)ì€ ì¼ë¶€ í™˜ê²½ì—ì„œ ë¶ˆì•ˆì •í•¨

ì°¸ê³ :
https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-use-audio-input-streams
"""
import asyncio
import logging
import tempfile
import os
import wave
import threading
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass

import azure.cognitiveservices.speech as speechsdk
from app.config import settings

logger = logging.getLogger(__name__)


@dataclass
class Utterance:
    """ë‹¨ì¼ ë°œí™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤."""
    speaker_id: int
    text: str
    start_time_ms: int
    end_time_ms: int
    confidence: float
    sequence_number: int


class DiarizationAgent:
    """
    Azure SpeechRecognizer ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì—ì´ì „íŠ¸.

    PushAudioInputStreamì„ ì‚¬ìš©í•˜ì—¬ Docker/AWS í™˜ê²½ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™.

    ì£¼ì˜: í™”ì ë¶„ë¦¬ ê¸°ëŠ¥ì€ íŠ¹ì • ë¦¬ì „ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥:
    - eastasia, southeastasia, centralus, eastus, westeurope
    - AZURE_AVATAR_SPEECH_KEY/REGION (southeastasia) ì‚¬ìš©
    """

    SUPPORTED_FORMATS = {'.wav', '.mp3', '.m4a', '.ogg', '.flac', '.webm'}

    def __init__(self):
        """Azure Speech ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”."""
        if not settings.AZURE_AVATAR_SPEECH_KEY:
            raise ValueError("AZURE_AVATAR_SPEECH_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (í™”ì ë¶„ë¦¬ì— í•„ìš”)")

        self.speech_key = settings.AZURE_AVATAR_SPEECH_KEY
        self.speech_region = settings.AZURE_AVATAR_SPEECH_REGION

        logger.info(f"DiarizationAgent ì´ˆê¸°í™” ì™„ë£Œ: ë¦¬ì „={self.speech_region}")

    async def process(
        self,
        audio_file_path: str,
        language: str = "en-US",
        progress_callback: Optional[Callable[[int, str], None]] = None
    ) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í™”ì ë¶„ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.

        Args:
            audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'en-US', 'ko-KR')
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (percent, message)

        Returns:
            {
                "utterances": [...],
                "speaker_count": 1,
                "duration_seconds": 180.5
            }
        """
        if not os.path.exists(audio_file_path):
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_file_path}")

        ext = os.path.splitext(audio_file_path)[1].lower()
        if ext not in self.SUPPORTED_FORMATS:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹: {ext}")

        if progress_callback:
            progress_callback(5, "ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ì¤‘...")

        # í•­ìƒ 16kHz/16bit/mono WAVë¡œ ë³€í™˜ (Docker í™˜ê²½ í˜¸í™˜ì„±)
        if progress_callback:
            progress_callback(10, "ì˜¤ë””ì˜¤ í¬ë§· ë³€í™˜ ì¤‘...")

        wav_path = await self._convert_to_wav(audio_file_path)

        try:
            if progress_callback:
                progress_callback(20, "ìŒì„± ì¸ì‹ ì‹œì‘...")

            result = await self._transcribe_with_push_stream(
                wav_path,
                language,
                progress_callback
            )

            if progress_callback:
                progress_callback(100, "ë¶„ì„ ì™„ë£Œ")

            return result

        finally:
            # ë³€í™˜ëœ ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if wav_path != audio_file_path and os.path.exists(wav_path):
                try:
                    os.remove(wav_path)
                    logger.debug(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {wav_path}")
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    async def _convert_to_wav(self, input_path: str) -> str:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ Azureìš© WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜.

        í•­ìƒ 16kHz, 16bit, mono PCMìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ë³´ì¥.
        """
        temp_fd, temp_path = tempfile.mkstemp(suffix='.wav')
        os.close(temp_fd)

        # Azure Speech SDK ìš”êµ¬ ì‚¬í•­: 16kHz, 16bit, mono PCM
        cmd = [
            'ffmpeg',
            '-i', input_path,
            '-ar', '16000',          # 16kHz ìƒ˜í”Œë ˆì´íŠ¸
            '-ac', '1',              # ëª¨ë…¸
            '-acodec', 'pcm_s16le',  # 16bit PCM little-endian
            '-y',                    # ë®ì–´ì“°ê¸°
            temp_path
        ]

        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.error(f"FFmpeg ì˜¤ë¥˜: {stderr.decode()}")
                raise ValueError("ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨")

            logger.info(f"ì˜¤ë””ì˜¤ ë³€í™˜ ì™„ë£Œ: {temp_path}")
            return temp_path

        except FileNotFoundError:
            raise ValueError("ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    async def _transcribe_with_push_stream(
        self,
        wav_path: str,
        language: str,
        progress_callback: Optional[Callable[[int, str], None]] = None
    ) -> Dict[str, Any]:
        """
        PushAudioInputStreamì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì¸ì‹ ìˆ˜í–‰.

        Docker/AWS í™˜ê²½ì—ì„œ AudioConfig(filename=...)ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆì–´
        ì§ì ‘ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        # WAV íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ì½ê¸°
        try:
            with wave.open(wav_path, 'rb') as wf:
                sample_rate = wf.getframerate()
                channels = wf.getnchannels()
                sample_width = wf.getsampwidth()
                n_frames = wf.getnframes()
                audio_data = wf.readframes(n_frames)

            logger.info(
                f"WAV íŒŒì¼ ì •ë³´: {sample_rate}Hz, {channels}ch, "
                f"{sample_width*8}bit, {len(audio_data)} bytes"
            )
        except Exception as e:
            logger.error(f"WAV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise ValueError(f"WAV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

        # PushAudioInputStream ì„¤ì • (16kHz, 16bit, mono)
        audio_format = speechsdk.audio.AudioStreamFormat(
            samples_per_second=16000,
            bits_per_sample=16,
            channels=1
        )
        push_stream = speechsdk.audio.PushAudioInputStream(stream_format=audio_format)
        audio_config = speechsdk.audio.AudioConfig(stream=push_stream)

        # Speech ì„¤ì • ìƒì„±
        speech_config = speechsdk.SpeechConfig(
            subscription=self.speech_key,
            region=self.speech_region
        )
        speech_config.speech_recognition_language = language
        speech_config.request_word_level_timestamps()
        speech_config.output_format = speechsdk.OutputFormat.Detailed

        # SpeechRecognizer ìƒì„±
        recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )

        # ê²°ê³¼ ì €ì¥ (thread-safe)
        utterances: List[Utterance] = []
        done = threading.Event()
        errors: List[str] = []
        lock = threading.Lock()

        def handle_recognized(evt):
            """ì¸ì‹ ì™„ë£Œ ì´ë²¤íŠ¸ ì²˜ë¦¬."""
            try:
                if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
                    text = evt.result.text
                    if text and text.strip():
                        offset_ticks = evt.result.offset
                        duration_ticks = evt.result.duration

                        # 100ë‚˜ë…¸ì´ˆ â†’ ë°€ë¦¬ì´ˆ
                        start_ms = offset_ticks // 10000
                        duration_ms = duration_ticks // 10000

                        with lock:
                            utterances.append(Utterance(
                                speaker_id=1,
                                text=text.strip(),
                                start_time_ms=start_ms,
                                end_time_ms=start_ms + duration_ms,
                                confidence=0.9,
                                sequence_number=len(utterances)
                            ))
                            count = len(utterances)

                        logger.info(f"âœ… ë°œí™” ì¸ì‹ [{start_ms}ms]: {text[:50]}...")

                        if progress_callback:
                            progress = min(90, 30 + count * 2)
                            progress_callback(progress, f"ë°œí™” {count}ê°œ ì¸ì‹ë¨...")

                elif evt.result.reason == speechsdk.ResultReason.NoMatch:
                    logger.debug(f"ë§¤ì¹­ ì—†ìŒ: {evt.result.no_match_details}")

            except Exception as e:
                logger.error(f"ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        def handle_canceled(evt):
            """ì·¨ì†Œ/ì˜¤ë¥˜ ì²˜ë¦¬."""
            logger.info(f"ğŸ”´ Canceled: reason={evt.reason}")

            if evt.reason == speechsdk.CancellationReason.Error:
                error_code = getattr(evt, 'error_code', 'Unknown')
                error_details = getattr(evt, 'error_details', 'No details')
                error_msg = f"ì½”ë“œ: {error_code}, ìƒì„¸: {error_details}"
                with lock:
                    errors.append(error_msg)
                logger.error(f"âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {error_msg}")
            elif evt.reason == speechsdk.CancellationReason.EndOfStream:
                logger.info("âœ… ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ (ì •ìƒ)")

            done.set()

        def handle_session_stopped(evt):
            """ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬."""
            logger.info("ğŸ”µ ì„¸ì…˜ ì¢…ë£Œë¨")
            done.set()

        def handle_session_started(evt):
            """ì„¸ì…˜ ì‹œì‘ ì²˜ë¦¬."""
            logger.info(f"ğŸŸ¢ ì„¸ì…˜ ì‹œì‘ë¨: {evt.session_id}")

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
        recognizer.recognized.connect(handle_recognized)
        recognizer.canceled.connect(handle_canceled)
        recognizer.session_stopped.connect(handle_session_stopped)
        recognizer.session_started.connect(handle_session_started)

        # ì²­í¬ í¬ê¸°: 3200 bytes = 100ms of audio at 16kHz, 16bit, mono
        chunk_size = 3200
        total_bytes = len(audio_data)

        logger.info(f"ğŸ“¤ ì˜¤ë””ì˜¤ ë°ì´í„° ì¤€ë¹„: {total_bytes} bytes")

        # ë¨¼ì € ì²« 1ì´ˆ(10 chunks) ì •ë„ í‘¸ì‹œí•˜ì—¬ ë²„í¼ ì±„ìš°ê¸°
        initial_chunks = min(total_bytes, chunk_size * 10)
        for i in range(0, initial_chunks, chunk_size):
            push_stream.write(audio_data[i:i + chunk_size])
        logger.info(f"ğŸ“¤ ì´ˆê¸° ë²„í¼ í‘¸ì‹œ: {initial_chunks} bytes")

        # continuous recognition ì‹œì‘
        logger.info(f"ğŸš€ ìŒì„± ì¸ì‹ ì‹œì‘ (PushStream): {wav_path}")
        recognizer.start_continuous_recognition()

        # ë‚˜ë¨¸ì§€ ë°ì´í„°ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í‘¸ì‹œ
        def push_remaining_data():
            """ë‚˜ë¨¸ì§€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¼ì— í‘¸ì‹œ."""
            try:
                pushed = initial_chunks
                for i in range(initial_chunks, total_bytes, chunk_size):
                    chunk = audio_data[i:i + chunk_size]
                    push_stream.write(chunk)
                    pushed += len(chunk)

                logger.info(f"ğŸ“¤ ì˜¤ë””ì˜¤ ë°ì´í„° í‘¸ì‹œ ì™„ë£Œ: {pushed} bytes")

                # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ (SDKì—ê²Œ ë°ì´í„° ëì„ ì•Œë¦¼)
                push_stream.close()
                logger.info("ğŸ“ª ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë‹«í˜")

            except Exception as e:
                logger.error(f"ì˜¤ë””ì˜¤ í‘¸ì‹œ ì˜¤ë¥˜: {e}")
                try:
                    push_stream.close()
                except:
                    pass

        push_thread = threading.Thread(target=push_remaining_data, daemon=True)
        push_thread.start()

        # ì™„ë£Œ ëŒ€ê¸° (threading.Event ì‚¬ìš©)
        completed = done.wait(timeout=600)  # ìµœëŒ€ 10ë¶„

        # í‘¸ì‹œ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        push_thread.join(timeout=5)

        if not completed:
            logger.warning("ìŒì„± ì¸ì‹ íƒ€ì„ì•„ì›ƒ (10ë¶„)")
            with lock:
                errors.append("íƒ€ì„ì•„ì›ƒ (10ë¶„)")

        # ì¸ì‹ ì¢…ë£Œ
        recognizer.stop_continuous_recognition()

        # ì—ëŸ¬ í™•ì¸
        with lock:
            if errors and "EndOfStream" not in str(errors):
                logger.error(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {errors}")

        # ê²°ê³¼ ê³„ì‚°
        with lock:
            result_duration = 0.0
            if utterances:
                result_duration = max(u.end_time_ms for u in utterances) / 1000.0

            unique_speakers = 1 if utterances else 0
            utterance_count = len(utterances)

            logger.info(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ: {utterance_count}ê°œ ë°œí™”, {result_duration:.1f}ì´ˆ")

            return {
                "utterances": [
                    {
                        "speaker_id": u.speaker_id,
                        "text": u.text,
                        "start_time_ms": u.start_time_ms,
                        "end_time_ms": u.end_time_ms,
                        "confidence": u.confidence,
                        "sequence_number": u.sequence_number
                    }
                    for u in utterances
                ],
                "speaker_count": unique_speakers,
                "duration_seconds": result_duration
            }

    def get_supported_languages(self) -> List[str]:
        """ì§€ì›ë˜ëŠ” ì–¸ì–´ ì½”ë“œ ëª©ë¡ ë°˜í™˜."""
        return [
            "en-US", "en-GB", "en-AU", "en-IN", "en-NZ", "en-CA",
            "ko-KR",
            "ja-JP",
            "zh-CN", "zh-TW", "zh-HK",
            "de-DE", "de-AT", "de-CH",
            "fr-FR", "fr-CA",
            "es-ES", "es-MX",
            "it-IT",
            "pt-BR", "pt-PT",
            "nl-NL",
            "ru-RU",
            "ar-SA", "ar-EG",
            "hi-IN",
            "th-TH",
            "vi-VN"
        ]
