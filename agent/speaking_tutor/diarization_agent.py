"""
Azure Speech SDKë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ ë° í™”ì ë¶„ë¦¬ ì—ì´ì „íŠ¸.

ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µ:
- í™”ìë³„ë¡œ ë¶„ë¦¬ëœ ë°œí™” ë‚´ìš©
- íƒ€ì„ìŠ¤íƒ¬í”„ (ì‹œì‘/ì¢…ë£Œ ì‹œê°„)
- ì‹ ë¢°ë„ ì ìˆ˜
- í™”ì ìˆ˜

ì°¸ê³ :
https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization
"""
import asyncio
import logging
import tempfile
import os
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass

import azure.cognitiveservices.speech as speechsdk
from app.config import settings

logger = logging.getLogger(__name__)


@dataclass
class Utterance:
    """ë‹¨ì¼ ë°œí™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤."""
    speaker_id: int
    text: str
    start_time_ms: int
    end_time_ms: int
    confidence: float
    sequence_number: int


class DiarizationAgent:
    """
    Azure SpeechRecognizer ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì—ì´ì „íŠ¸.

    SpeechRecognizer + continuous_recognitionì„ ì‚¬ìš©í•˜ì—¬
    íŒŒì¼ ê¸°ë°˜ ìŒì„± ì¸ì‹ ë° í™”ì ë¶„ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    ì£¼ì˜: í™”ì ë¶„ë¦¬ ê¸°ëŠ¥ì€ íŠ¹ì • ë¦¬ì „ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥:
    - eastasia, southeastasia, centralus, eastus, westeurope
    - AZURE_AVATAR_SPEECH_KEY/REGION (southeastasia) ì‚¬ìš©
    """

    SUPPORTED_FORMATS = {'.wav', '.mp3', '.m4a', '.ogg', '.flac', '.webm'}

    def __init__(self):
        """Azure Speech ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”."""
        # í™”ì ë¶„ë¦¬ë¥¼ ìœ„í•´ Avatar í‚¤/ë¦¬ì „ ì‚¬ìš©
        # koreacentralì€ í™”ì ë¶„ë¦¬ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
        if not settings.AZURE_AVATAR_SPEECH_KEY:
            raise ValueError("AZURE_AVATAR_SPEECH_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (í™”ì ë¶„ë¦¬ì— í•„ìš”)")

        self.speech_key = settings.AZURE_AVATAR_SPEECH_KEY
        self.speech_region = settings.AZURE_AVATAR_SPEECH_REGION

        logger.info(f"DiarizationAgent ì´ˆê¸°í™” ì™„ë£Œ: ë¦¬ì „={self.speech_region}")

    async def process(
        self,
        audio_file_path: str,
        language: str = "en-US",
        progress_callback: Optional[Callable[[int, str], None]] = None
    ) -> Dict[str, Any]:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í™”ì ë¶„ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.

        Args:
            audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'en-US', 'ko-KR')
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (percent, message)

        Returns:
            {
                "utterances": [...],
                "speaker_count": 3,
                "duration_seconds": 180.5
            }
        """
        if not os.path.exists(audio_file_path):
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_file_path}")

        ext = os.path.splitext(audio_file_path)[1].lower()
        if ext not in self.SUPPORTED_FORMATS:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹: {ext}")

        if progress_callback:
            progress_callback(5, "ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ì¤‘...")

        # WAVê°€ ì•„ë‹ˆë©´ ë³€í™˜ í•„ìš”
        wav_path = audio_file_path
        temp_wav = None

        if ext != '.wav':
            if progress_callback:
                progress_callback(10, "ì˜¤ë””ì˜¤ í¬ë§· ë³€í™˜ ì¤‘...")
            wav_path = await self._convert_to_wav(audio_file_path)
            temp_wav = wav_path

        try:
            if progress_callback:
                progress_callback(20, "ìŒì„± ì¸ì‹ ì‹œì‘...")

            result = await self._transcribe_with_speech_recognizer(
                wav_path,
                language,
                progress_callback
            )

            if progress_callback:
                progress_callback(100, "ë¶„ì„ ì™„ë£Œ")

            return result

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if temp_wav and os.path.exists(temp_wav):
                try:
                    os.remove(temp_wav)
                except Exception as e:
                    logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    async def _convert_to_wav(self, input_path: str) -> str:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ Azureìš© WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
        temp_fd, temp_path = tempfile.mkstemp(suffix='.wav')
        os.close(temp_fd)

        cmd = [
            'ffmpeg',
            '-i', input_path,
            '-ar', '16000',       # 16kHz ìƒ˜í”Œë ˆì´íŠ¸
            '-ac', '1',           # ëª¨ë…¸
            '-acodec', 'pcm_s16le',  # 16bit PCM
            '-y',
            temp_path
        ]

        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.error(f"FFmpeg ì˜¤ë¥˜: {stderr.decode()}")
                raise ValueError("ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨")

            logger.info(f"ì˜¤ë””ì˜¤ ë³€í™˜ ì™„ë£Œ: {temp_path}")
            return temp_path

        except FileNotFoundError:
            raise ValueError("ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    async def _transcribe_with_speech_recognizer(
        self,
        wav_path: str,
        language: str,
        progress_callback: Optional[Callable[[int, str], None]] = None
    ) -> Dict[str, Any]:
        """
        SpeechRecognizer + continuous_recognitionìœ¼ë¡œ ìŒì„± ì¸ì‹ ìˆ˜í–‰.

        íŒŒì¼ ê¸°ë°˜ ìŒì„± ì¸ì‹ì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ ë°©ì‹ì…ë‹ˆë‹¤.
        """
        # Speech ì„¤ì • ìƒì„±
        speech_config = speechsdk.SpeechConfig(
            subscription=self.speech_key,
            region=self.speech_region
        )
        speech_config.speech_recognition_language = language

        # ë‹¨ì–´ ìˆ˜ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„ ìš”ì²­
        speech_config.request_word_level_timestamps()

        # ì¶œë ¥ í˜•ì‹ ì„¤ì • (ìƒì„¸ ì •ë³´ í¬í•¨)
        speech_config.output_format = speechsdk.OutputFormat.Detailed

        # ì˜¤ë””ì˜¤ ì„¤ì • (íŒŒì¼ì—ì„œ ì§ì ‘ ì½ê¸°)
        audio_config = speechsdk.AudioConfig(filename=wav_path)

        # SpeechRecognizer ìƒì„±
        recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )

        # ê²°ê³¼ ì €ì¥
        utterances: List[Utterance] = []
        done = asyncio.Event()
        errors: List[str] = []

        def handle_recognized(evt):
            """ì¸ì‹ ì™„ë£Œ ì´ë²¤íŠ¸ ì²˜ë¦¬."""
            try:
                if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
                    text = evt.result.text
                    if text and text.strip():
                        # íƒ€ì´ë° ì •ë³´ ì¶”ì¶œ
                        offset_ticks = evt.result.offset
                        duration_ticks = evt.result.duration

                        # 100ë‚˜ë…¸ì´ˆ â†’ ë°€ë¦¬ì´ˆ
                        start_ms = offset_ticks // 10000
                        duration_ms = duration_ticks // 10000

                        # í™”ì IDëŠ” ë‹¨ìˆœí•˜ê²Œ 1ë¡œ ì„¤ì • (ë‹¨ì¼ í™”ì ê°€ì •)
                        # ì‹¤ì œ í™”ì ë¶„ë¦¬ëŠ” Azureì˜ ì œí•œìœ¼ë¡œ ì–´ë ¤ì›€
                        speaker_id = 1

                        utterances.append(Utterance(
                            speaker_id=speaker_id,
                            text=text.strip(),
                            start_time_ms=start_ms,
                            end_time_ms=start_ms + duration_ms,
                            confidence=0.9,
                            sequence_number=len(utterances)
                        ))

                        logger.info(f"âœ… ë°œí™” ì¸ì‹: {text[:50]}...")

                        if progress_callback:
                            progress = min(90, 30 + len(utterances) * 2)
                            progress_callback(progress, f"ë°œí™” {len(utterances)}ê°œ ì¸ì‹ë¨...")

                elif evt.result.reason == speechsdk.ResultReason.NoMatch:
                    logger.debug(f"ë§¤ì¹­ ì—†ìŒ: {evt.result.no_match_details}")

            except Exception as e:
                logger.error(f"ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        def handle_canceled(evt):
            """ì·¨ì†Œ/ì˜¤ë¥˜ ì²˜ë¦¬."""
            logger.info(f"ğŸ”´ Canceled: reason={evt.reason}")

            if evt.reason == speechsdk.CancellationReason.Error:
                error_code = getattr(evt, 'error_code', 'Unknown')
                error_details = getattr(evt, 'error_details', 'No details')
                error_msg = f"ì½”ë“œ: {error_code}, ìƒì„¸: {error_details}"
                errors.append(error_msg)
                logger.error(f"âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {error_msg}")
            elif evt.reason == speechsdk.CancellationReason.EndOfStream:
                logger.info("âœ… ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")

            done.set()

        def handle_session_stopped(evt):
            """ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬."""
            logger.info("ğŸ”µ ì„¸ì…˜ ì¢…ë£Œë¨")
            done.set()

        def handle_session_started(evt):
            """ì„¸ì…˜ ì‹œì‘ ì²˜ë¦¬."""
            logger.info(f"ğŸŸ¢ ì„¸ì…˜ ì‹œì‘ë¨: {evt.session_id}")

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
        recognizer.recognized.connect(handle_recognized)
        recognizer.canceled.connect(handle_canceled)
        recognizer.session_stopped.connect(handle_session_stopped)
        recognizer.session_started.connect(handle_session_started)

        # continuous recognition ì‹œì‘
        logger.info(f"ğŸš€ ìŒì„± ì¸ì‹ ì‹œì‘: {wav_path}")
        recognizer.start_continuous_recognition()

        # ì™„ë£Œ ëŒ€ê¸°
        try:
            await asyncio.wait_for(done.wait(), timeout=600)  # ìµœëŒ€ 10ë¶„
        except asyncio.TimeoutError:
            logger.warning("ìŒì„± ì¸ì‹ íƒ€ì„ì•„ì›ƒ (10ë¶„)")
            errors.append("íƒ€ì„ì•„ì›ƒ (10ë¶„)")

        # ì¸ì‹ ì¢…ë£Œ
        recognizer.stop_continuous_recognition()

        if errors:
            logger.error(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {errors}")

        # ê²°ê³¼ ê³„ì‚°
        result_duration = 0.0
        if utterances:
            result_duration = max(u.end_time_ms for u in utterances) / 1000.0

        # í™”ì ìˆ˜ (í˜„ì¬ëŠ” ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬)
        unique_speakers = 1 if utterances else 0

        logger.info(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ: {len(utterances)}ê°œ ë°œí™”, {result_duration:.1f}ì´ˆ")

        return {
            "utterances": [
                {
                    "speaker_id": u.speaker_id,
                    "text": u.text,
                    "start_time_ms": u.start_time_ms,
                    "end_time_ms": u.end_time_ms,
                    "confidence": u.confidence,
                    "sequence_number": u.sequence_number
                }
                for u in utterances
            ],
            "speaker_count": unique_speakers,
            "duration_seconds": result_duration
        }

    def get_supported_languages(self) -> List[str]:
        """ì§€ì›ë˜ëŠ” ì–¸ì–´ ì½”ë“œ ëª©ë¡ ë°˜í™˜."""
        return [
            "en-US", "en-GB", "en-AU", "en-IN", "en-NZ", "en-CA",
            "ko-KR",
            "ja-JP",
            "zh-CN", "zh-TW", "zh-HK",
            "de-DE", "de-AT", "de-CH",
            "fr-FR", "fr-CA",
            "es-ES", "es-MX",
            "it-IT",
            "pt-BR", "pt-PT",
            "nl-NL",
            "ru-RU",
            "ar-SA", "ar-EG",
            "hi-IN",
            "th-TH",
            "vi-VN"
        ]
