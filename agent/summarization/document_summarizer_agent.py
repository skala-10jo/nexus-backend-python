"""
ë¬¸ì„œ ìš”ì•½ Agent (Document Summarizer Agent)

ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì—¬ ë²ˆì—­ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” Micro Agent.
ë‹¨ì¼ ì±…ì„: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ â†’ í•µì‹¬ ìš”ì•½

ì¬ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
- ë²ˆì—­ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: í”„ë¡œì íŠ¸ ë¬¸ì„œ ìš”ì•½
- ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: ê¸´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©
- ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ìš”ì•½
- í”„ë¡œì íŠ¸ ê°œìš” ìƒì„±: í”„ë¡œì íŠ¸ ì„¤ëª… ìë™ ìƒì„±
"""

import logging
from typing import List
from agent.base_agent import BaseAgent

logger = logging.getLogger(__name__)


class DocumentSummarizerAgent(BaseAgent):
    """
    ë¬¸ì„œ ìš”ì•½ Agent.

    ì±…ì„: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ â†’ í•µì‹¬ ìš”ì•½

    ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì£¼ì œì™€ ë§¥ë½ì„ ìš”ì•½í•©ë‹ˆë‹¤.
    ë²ˆì—­ ì‹œ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ê±°ë‚˜ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        >>> agent = DocumentSummarizerAgent()
        >>> documents = [
        ...     "í´ë¼ìš°ë“œ ì»´í“¨íŒ… ì•„í‚¤í…ì²˜ ë¬¸ì„œ...",
        ...     "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì„¤ê³„ ê°€ì´ë“œ..."
        ... ]
        >>> summary = await agent.process(documents, max_length=500)
        >>> print(summary)
        "ì´ í”„ë¡œì íŠ¸ëŠ” í´ë¼ìš°ë“œ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤..."
    """

    def _create_system_prompt(self) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Returns:
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        return """ë‹¹ì‹ ì€ ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì£¼ì œì™€ ë§¥ë½ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.

**ìš”ì•½ ì›ì¹™**:
1. ë¬¸ì„œë“¤ì˜ ê³µí†µ ì£¼ì œì™€ í•µì‹¬ ë‚´ìš© íŒŒì•…
2. ì „ë¬¸ ìš©ì–´ì™€ ê¸°ìˆ ì  ë§¥ë½ ìœ ì§€
3. í”„ë¡œì íŠ¸ì˜ ë„ë©”ì¸ê³¼ ëª©ì  ëª…í™•íˆ í‘œí˜„
4. ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ìƒëµ

**ì¶œë ¥ í˜•ì‹**:
- 2-3ê°œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±
- ê° ë¬¸ë‹¨ì€ 2-3ë¬¸ì¥
- ëª…í™•í•˜ê³  ê°„ê²°í•œ ë¬¸ì¥ ì‚¬ìš©
- ì „ë¬¸ ìš©ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€

**ì¤‘ìš”**:
- ìš”ì•½ë¬¸ë§Œ ì¶œë ¥í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ê°ê´€ì ì´ê³  ì‚¬ì‹¤ì ì¸ ìš”ì•½ì„ ì œê³µí•˜ì„¸ìš”
"""

    async def process(
        self,
        documents: List[str],
        max_length: int = 500
    ) -> str:
        """
        ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            documents: ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´ (ë¬¸ì ìˆ˜, ê¸°ë³¸ê°’: 500)

        Returns:
            ìš”ì•½ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸

        Raises:
            ValueError: ë¬¸ì„œê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
            Exception: GPT API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        if not documents:
            raise ValueError("ìš”ì•½í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")

        # ë¹ˆ ë¬¸ì„œ ì œê±°
        valid_documents = [doc.strip() for doc in documents if doc and doc.strip()]

        if not valid_documents:
            raise ValueError("ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.info(f"ğŸ“ ë¬¸ì„œ ìš”ì•½ ì‹œì‘: {len(valid_documents)}ê°œ ë¬¸ì„œ")

        try:
            # ë¬¸ì„œë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            combined_text = "\n\n---\n\n".join(valid_documents)
            
            # 50,000ì ì´í•˜: í•œ ë²ˆì— ì²˜ë¦¬
            # 50,000ì ì´ˆê³¼: ì²­í¬ ë¶„í•  â†’ ê°œë³„ ìš”ì•½ â†’ ìµœì¢… í†µí•©
            MAX_CHARS = 50000
            
            if len(combined_text) <= MAX_CHARS:
                # ë‹¨ì¼ ìš”ì•½
                logger.info(f"ğŸ“„ ë‹¨ì¼ ìš”ì•½ ëª¨ë“œ: {len(combined_text)}ì")
                summary = await self._summarize_text(combined_text, max_length)
            else:
                # ì²­í¬ ë¶„í•  ìš”ì•½ (Map-Reduce)
                logger.info(f"ğŸ“š ì²­í¬ ë¶„í•  ëª¨ë“œ: {len(combined_text)}ì â†’ ì²­í¬ ë¶„í• ")
                summary = await self._summarize_with_chunks(combined_text, max_length, MAX_CHARS)

            logger.info(f"âœ… ìš”ì•½ ì™„ë£Œ: {len(combined_text)}ì â†’ {len(summary)}ì")
            return summary

        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë¬¸ì„œ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    async def _summarize_text(self, text: str, max_length: int) -> str:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

        Args:
            text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´

        Returns:
            ìš”ì•½ëœ í…ìŠ¤íŠ¸
        """
        system_prompt = self._create_system_prompt()
        user_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”:

{text}
"""

        response = await self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        return response.choices[0].message.content.strip()

    async def _summarize_with_chunks(
        self,
        text: str,
        max_length: int,
        chunk_size: int
    ) -> str:
        """
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤ (Map-Reduce íŒ¨í„´).

        1ë‹¨ê³„ (Map): ê° ì²­í¬ë¥¼ ê°œë³„ ìš”ì•½
        2ë‹¨ê³„ (Reduce): ê°œë³„ ìš”ì•½ë“¤ì„ ìµœì¢… í†µí•© ìš”ì•½

        Args:
            text: ì „ì²´ í…ìŠ¤íŠ¸
            max_length: ìµœì¢… ìš”ì•½ ìµœëŒ€ ê¸¸ì´
            chunk_size: ì²­í¬ í¬ê¸°

        Returns:
            ìµœì¢… í†µí•© ìš”ì•½
        """
        # 1. í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = self._split_into_chunks(text, chunk_size)
        logger.info(f"ğŸ“¦ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

        # 2. ê° ì²­í¬ ê°œë³„ ìš”ì•½ (Map ë‹¨ê³„)
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ğŸ”„ ì²­í¬ {i+1}/{len(chunks)} ìš”ì•½ ì¤‘...")
            chunk_summary = await self._summarize_text(chunk, max_length=500)
            chunk_summaries.append(chunk_summary)

        # 3. ê°œë³„ ìš”ì•½ë“¤ì„ í†µí•© (Reduce ë‹¨ê³„)
        logger.info(f"ğŸ”— {len(chunk_summaries)}ê°œ ìš”ì•½ í†µí•© ì¤‘...")
        combined_summaries = "\n\n---\n\n".join(chunk_summaries)

        # í†µí•©ëœ ìš”ì•½ì´ ì—¬ì „íˆ ê¸¸ë©´ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
        if len(combined_summaries) > chunk_size:
            return await self._summarize_with_chunks(combined_summaries, max_length, chunk_size)

        # ìµœì¢… ìš”ì•½ ìƒì„±
        final_summary = await self._summarize_text(combined_summaries, max_length)
        return final_summary

    def _split_into_chunks(self, text: str, chunk_size: int) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        ë¬¸ë‹¨ ê²½ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°

        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        current_pos = 0
        text_length = len(text)

        while current_pos < text_length:
            # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
            end_pos = min(current_pos + chunk_size, text_length)

            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ë‹¨ ê²½ê³„ ì°¾ê¸°
            if end_pos < text_length:
                # ê°€ê¹Œìš´ ë¬¸ë‹¨ ê²½ê³„ ì°¾ê¸° (ì—­ìˆœìœ¼ë¡œ)
                boundary_pos = text.rfind("\n\n", current_pos, end_pos)
                if boundary_pos > current_pos:
                    end_pos = boundary_pos

            chunk = text[current_pos:end_pos].strip()
            if chunk:
                chunks.append(chunk)

            current_pos = end_pos
            # ë¬¸ë‹¨ ê²½ê³„ì—ì„œ ë¶„í• í–ˆìœ¼ë©´ ê³µë°± ê±´ë„ˆë›°ê¸°
            while current_pos < text_length and text[current_pos] in "\n\r\t ":
                current_pos += 1

        return chunks
