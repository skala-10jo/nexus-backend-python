"""
ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ Agent (Context-Enhanced Translation Agent)

í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ì™€ ìš©ì–´ì§‘ì„ í™œìš©í•œ ê³ í’ˆì§ˆ ë²ˆì—­ Micro Agent.
ë‹¨ì¼ ì±…ì„: í…ìŠ¤íŠ¸ + ì»¨í…ìŠ¤íŠ¸ + ìš©ì–´ì§‘ â†’ ì •í™•í•œ ì „ë¬¸ ë²ˆì—­

ì¬ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
- í”„ë¡œì íŠ¸ ê¸°ë°˜ ë²ˆì—­: í”„ë¡œì íŠ¸ ë¬¸ì„œì™€ ìš©ì–´ì§‘ í™œìš©
- ë¬¸ì„œ ë²ˆì—­: ì „ë¬¸ ë¶„ì•¼ ë¬¸ì„œì˜ ì •í™•í•œ ë²ˆì—­
- ì „ë¬¸ ë¶„ì•¼ ë²ˆì—­: ì˜ë£Œ, ë²•ë¥ , ê¸°ìˆ  ë“± ë„ë©”ì¸ íŠ¹í™” ë²ˆì—­
- ì¼ê´€ì„± ìˆëŠ” ë²ˆì—­: ìš©ì–´ í†µì¼ ë° ìŠ¤íƒ€ì¼ ìœ ì§€
"""

import logging
from typing import List, Dict
from agent.base_agent import BaseAgent
from agent.term_detection.term_detector_agent import DetectedTerm

logger = logging.getLogger(__name__)


class ContextEnhancedTranslationAgent(BaseAgent):
    """
    ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê³ ê¸‰ ë²ˆì—­ Agent

    ì±…ì„: í…ìŠ¤íŠ¸ + ì»¨í…ìŠ¤íŠ¸ + ìš©ì–´ì§‘ + íƒì§€ëœ ìš©ì–´ â†’ ì •í™•í•œ ë²ˆì—­

    í”„ë¡œì íŠ¸ ë¬¸ì„œì˜ ë§¥ë½ê³¼ ì „ë¬¸ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬
    ì¼ë°˜ ë²ˆì—­ë³´ë‹¤ í›¨ì”¬ ì •í™•í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ë²ˆì—­ì„ ì œê³µí•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        >>> agent = ContextEnhancedTranslationAgent()
        >>> result = await agent.process(
        ...     text="í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì»¨í…Œì´ë„ˆë¥¼ ë°°í¬í•©ë‹ˆë‹¤",
        ...     source_lang="ko",
        ...     target_lang="en",
        ...     context="ì´ í”„ë¡œì íŠ¸ëŠ” AWS ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤...",
        ...     glossary_terms=[{"korean_term": "ì»¨í…Œì´ë„ˆ", "english_term": "Container"}],
        ...     detected_terms=[...]
        ... )
        >>> print(result)
        "Deploy containers in the cloud environment"
    """

    def _create_system_prompt(
        self,
        source_lang: str,
        target_lang: str,
        context: str,
        glossary_terms: List[Dict[str, str]],
        detected_terms: List[DetectedTerm]
    ) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ
            target_lang: ëª©í‘œ ì–¸ì–´ ì½”ë“œ
            context: í”„ë¡œì íŠ¸/ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            glossary_terms: ì „ì²´ ìš©ì–´ì§‘
            detected_terms: í…ìŠ¤íŠ¸ì—ì„œ íƒì§€ëœ ìš©ì–´

        Returns:
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        lang_names = {
            "ko": "í•œêµ­ì–´",
            "en": "ì˜ì–´",
            "ja": "ì¼ë³¸ì–´",
            "vi": "ë² íŠ¸ë‚¨ì–´",
            "zh": "ì¤‘êµ­ì–´"
        }

        source_name = lang_names.get(source_lang, source_lang)
        target_name = lang_names.get(target_lang, target_lang)

        # íƒì§€ëœ ìš©ì–´ í¬ë§·íŒ…
        detected_terms_text = ""
        if detected_terms:
            detected_terms_text = "\n**ë²ˆì—­í•  í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ì „ë¬¸ìš©ì–´**:\n"
            for term in detected_terms:
                english = term.english_term or ""
                vietnamese = term.vietnamese_term or ""
                detected_terms_text += f"- {term.korean_term}"
                if english:
                    detected_terms_text += f" â†’ {english}"
                if vietnamese:
                    detected_terms_text += f" (ë² íŠ¸ë‚¨ì–´: {vietnamese})"
                detected_terms_text += "\n"

        # ì£¼ìš” ìš©ì–´ í¬ë§·íŒ… (ìµœëŒ€ 20ê°œ)
        glossary_text = ""
        if glossary_terms:
            glossary_text = "\n**ì „ë¬¸ìš©ì–´ì‚¬ì „ (ì°¸ê³ ìš©)**:\n"
            for i, term in enumerate(glossary_terms[:20]):
                korean = term.get("korean_term", "")
                english = term.get("english_term", "")
                vietnamese = term.get("vietnamese_term", "")
                if korean:
                    glossary_text += f"- {korean}"
                    if english:
                        glossary_text += f" â†’ {english}"
                    if vietnamese:
                        glossary_text += f" (ë² íŠ¸ë‚¨ì–´: {vietnamese})"
                    glossary_text += "\n"

            if len(glossary_terms) > 20:
                glossary_text += f"... ì™¸ {len(glossary_terms) - 20}ê°œ ìš©ì–´\n"

        return f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì•¼ ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í”„ë¡œì íŠ¸ ë§¥ë½ê³¼ ì „ë¬¸ìš©ì–´ì‚¬ì „ì„ ì°¸ê³ í•˜ì—¬ {source_name} í…ìŠ¤íŠ¸ë¥¼ {target_name}ë¡œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•˜ì„¸ìš”.

**í”„ë¡œì íŠ¸ ë§¥ë½**:
{context}
{detected_terms_text}{glossary_text}

**ë²ˆì—­ ì›ì¹™**:
1. **ì „ë¬¸ìš©ì–´ ìš°ì„  ì ìš©**: ìœ„ì— í‘œì‹œëœ ì „ë¬¸ìš©ì–´ëŠ” ìš©ì–´ì§‘ì˜ ë²ˆì—­ì„ **ë°˜ë“œì‹œ** ì‚¬ìš©í•˜ì„¸ìš”
2. **ë§¥ë½ ê³ ë ¤**: í”„ë¡œì íŠ¸ì˜ ê¸°ìˆ  ìŠ¤íƒê³¼ ë„ë©”ì¸ì„ ê³ ë ¤í•˜ì—¬ ë²ˆì—­
3. **ì¼ê´€ì„± ìœ ì§€**: ê°™ì€ ìš©ì–´ëŠ” í•­ìƒ ê°™ì€ ë²ˆì—­ ì‚¬ìš©
4. **ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„**: ëª©í‘œ ì–¸ì–´ì˜ ê´€ìš©ì  í‘œí˜„ ì‚¬ìš©
5. **ê²©ì‹ ìœ ì§€**: ì›ë¬¸ì˜ í†¤ê³¼ ê²©ì‹ ìˆ˜ì¤€ ìœ ì§€

**ì¤‘ìš”**:
- íƒì§€ëœ ì „ë¬¸ìš©ì–´ëŠ” **ë°˜ë“œì‹œ** ìš©ì–´ì§‘ì˜ ë²ˆì—­ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì£¼ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ì „ë¬¸ìš©ì–´ê°€ ì•„ë‹Œ ì¼ë°˜ ë‹¨ì–´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•˜ì„¸ìš”
"""

    async def process(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        context: str,
        glossary_terms: List[Dict[str, str]],
        detected_terms: List[DetectedTerm]
    ) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ ìˆ˜í–‰

        Args:
            text: ë²ˆì—­í•  ì›ë¬¸
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ko, en, ja, vi ë“±)
            target_lang: ëª©í‘œ ì–¸ì–´ ì½”ë“œ
            context: í”„ë¡œì íŠ¸/ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (DocumentSummarizerAgent ê²°ê³¼)
            glossary_terms: í”„ë¡œì íŠ¸ ìš©ì–´ì§‘
            detected_terms: í…ìŠ¤íŠ¸ì—ì„œ íƒì§€ëœ ìš©ì–´ (TermDetectorAgent ê²°ê³¼)

        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸

        Raises:
            ValueError: ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
            Exception: GPT API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        if not text or not text.strip():
            raise ValueError("ë²ˆì—­í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        if source_lang == target_lang:
            raise ValueError("ì›ë³¸ ì–¸ì–´ì™€ ëª©í‘œ ì–¸ì–´ê°€ ë™ì¼í•©ë‹ˆë‹¤")

        if not context:
            logger.warning("âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë²ˆì—­ìœ¼ë¡œ ëŒ€ì²´ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")

        logger.info(f"ğŸŒ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ ì‹œì‘: {source_lang} â†’ {target_lang}")
        logger.info(f"ğŸ“š ìš©ì–´ì§‘: {len(glossary_terms)}ê°œ, íƒì§€ëœ ìš©ì–´: {len(detected_terms)}ê°œ")

        try:
            system_prompt = self._create_system_prompt(
                source_lang,
                target_lang,
                context,
                glossary_terms,
                detected_terms
            )

            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                temperature=0.2,  # ë” ì¼ê´€ì„± ìˆëŠ” ë²ˆì—­
                max_tokens=2000
            )

            translated_text = response.choices[0].message.content.strip()

            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(text)}ì â†’ {len(translated_text)}ì")

            return translated_text

        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
