"""
ìµœì í™”ëœ ìš©ì–´ íƒì§€ Agent (Optimized Term Detector Agent)

Aho-Corasick ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê³ ì„±ëŠ¥ ìš©ì–´ íƒì§€ë¥¼ ìˆ˜í–‰í•˜ëŠ” Micro Agent.
ë‹¨ì¼ ì±…ì„: í…ìŠ¤íŠ¸ + ìš©ì–´ì§‘ â†’ íƒì§€ëœ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ (O(M+Z) ì‹œê°„ë³µì¡ë„)

ì„±ëŠ¥ ê°œì„ :
- ê¸°ì¡´: O(N Ã— M) - N=ìš©ì–´ ìˆ˜, M=í…ìŠ¤íŠ¸ ê¸¸ì´
- ê°œì„ : O(M + Z) - M=í…ìŠ¤íŠ¸ ê¸¸ì´, Z=ë§¤ì¹­ ìˆ˜
- 5,000 ìš©ì–´ Ã— 5,000ì ê¸°ì¤€ ì•½ 5,000ë°° ì„±ëŠ¥ í–¥ìƒ

ë„ì–´ì“°ê¸° ì •ê·œí™”:
- "ì¸ê³µ ì§€ëŠ¥" â†” "ì¸ê³µì§€ëŠ¥" ë§¤ì¹­ ì§€ì›
- ì›ë³¸ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì—­ì‚° ê¸°ëŠ¥
- ì–¸ì–´ë³„ ì •ê·œí™” ì „ëµ ì ìš©

ì¬ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
- ë²ˆì—­: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ì„ ìœ„í•œ ìš©ì–´ íƒì§€
- ë¬¸ì„œ ë¶„ì„: ë¬¸ì„œì—ì„œ ì‚¬ìš©ëœ ì „ë¬¸ìš©ì–´ ë¶„ì„
- í’ˆì§ˆ ê²€ì¦: ë²ˆì—­ í’ˆì§ˆ ì²´í¬
- ìë™ ì™„ì„±: ì‚¬ìš©ì ì…ë ¥ ì‹œ ìš©ì–´ ì œì•ˆ
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Tuple
import logging
import hashlib
from agent.base_agent import BaseAgent

try:
    import ahocorasick
    AHOCORASICK_AVAILABLE = True
except ImportError:
    AHOCORASICK_AVAILABLE = False
    logging.warning("âš ï¸ pyahocorasick not installed. Falling back to regex-based matching.")

logger = logging.getLogger(__name__)


@dataclass
class DetectedTerm:
    """
    íƒì§€ëœ ìš©ì–´ ì •ë³´

    Attributes:
        matched_text: ë¬¸ì„œì—ì„œ ì‹¤ì œ ë§¤ì¹­ëœ í…ìŠ¤íŠ¸
        position_start: ì›ë¬¸ì—ì„œì˜ ì‹œì‘ ìœ„ì¹˜ (ì¸ë±ìŠ¤)
        position_end: ì›ë¬¸ì—ì„œì˜ ì¢…ë£Œ ìœ„ì¹˜ (ì¸ë±ìŠ¤)
        korean_term: ìš©ì–´ì§‘ì˜ í•œê¸€ ìš©ì–´
        english_term: ìš©ì–´ì§‘ì˜ ì˜ì–´ ìš©ì–´ (ìˆëŠ” ê²½ìš°)
        vietnamese_term: ìš©ì–´ì§‘ì˜ ë² íŠ¸ë‚¨ì–´ ìš©ì–´ (ìˆëŠ” ê²½ìš°)
    """
    matched_text: str
    position_start: int
    position_end: int
    korean_term: str
    english_term: Optional[str] = None
    vietnamese_term: Optional[str] = None


@dataclass
class PositionMapping:
    """
    ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ì™€ ì›ë³¸ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ„ì¹˜ ë§¤í•‘

    ë„ì–´ì“°ê¸° ì •ê·œí™” ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ì—­ì‚°í•˜ê¸° ìœ„í•œ ë§¤í•‘ ì •ë³´.

    Attributes:
        normalized_text: ê³µë°±ì´ ì œê±°ëœ ì •ê·œí™” í…ìŠ¤íŠ¸
        original_text: ì›ë³¸ í…ìŠ¤íŠ¸
        norm_to_orig: ì •ê·œí™” ì¸ë±ìŠ¤ â†’ ì›ë³¸ ì¸ë±ìŠ¤ ë§¤í•‘ ë¦¬ìŠ¤íŠ¸
            norm_to_orig[i] = ì •ê·œí™” í…ìŠ¤íŠ¸ì˜ ië²ˆì§¸ ë¬¸ìê°€ ì›ë³¸ì˜ ëª‡ ë²ˆì§¸ ìœ„ì¹˜ì¸ì§€

    Example:
        >>> text = "ì¸ê³µ ì§€ëŠ¥ê³¼ ë¨¸ì‹  ëŸ¬ë‹"
        >>> mapping = PositionMapping(...)
        >>> mapping.normalized_text
        'ì¸ê³µì§€ëŠ¥ê³¼ë¨¸ì‹ ëŸ¬ë‹'
        >>> mapping.norm_to_orig
        [0, 1, 3, 4, 5, 7, 8, 10, 11]
    """
    normalized_text: str
    original_text: str
    norm_to_orig: List[int] = field(default_factory=list)


class AutomatonCache:
    """
    Aho-Corasick Automaton ìºì‹œ

    í”„ë¡œì íŠ¸ë³„ ìš©ì–´ì§‘ì— ëŒ€í•œ automatonì„ ìºì‹±í•˜ì—¬
    ë°˜ë³µì ì¸ automaton ë¹Œë“œ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

    ìºì‹œ í‚¤ëŠ” ìš©ì–´ì§‘ í•´ì‹œê°’ì„ ì‚¬ìš©í•˜ì—¬ ìš©ì–´ì§‘ ë³€ê²½ ì‹œ ìë™ ë¬´íš¨í™”ë©ë‹ˆë‹¤.
    """

    def __init__(self, max_size: int = 100):
        """
        Args:
            max_size: ìµœëŒ€ ìºì‹œ í¬ê¸° (LRU ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬)
        """
        self._cache: Dict[str, Any] = {}
        self._access_order: List[str] = []
        self._max_size = max_size

    def _generate_cache_key(
        self,
        glossary_terms: List[Dict],
        lang_field: str,
        normalize_mode: bool = False
    ) -> str:
        """
        ìš©ì–´ì§‘ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±

        ìš©ì–´ì§‘ ë‚´ìš©ì´ ë³€ê²½ë˜ë©´ ë‹¤ë¥¸ í•´ì‹œê°€ ìƒì„±ë˜ì–´
        ìë™ìœ¼ë¡œ ìºì‹œê°€ ë¬´íš¨í™”ë©ë‹ˆë‹¤.

        Args:
            glossary_terms: ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸
            lang_field: ì–¸ì–´ í•„ë“œëª…
            normalize_mode: ì •ê·œí™” ëª¨ë“œ ì—¬ë¶€ (Trueë©´ '_norm' ì ‘ë¯¸ì‚¬ ì¶”ê°€)

        Returns:
            ìºì‹œ í‚¤ ë¬¸ìì—´
        """
        # ìš©ì–´ì§‘ì˜ í•´ë‹¹ ì–¸ì–´ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ í•´ì‹œ ìƒì„±
        terms_str = "|".join([
            str(t.get(lang_field, ""))
            for t in glossary_terms
            if t.get(lang_field)
        ])
        mode_suffix = "_norm" if normalize_mode else ""
        hash_value = hashlib.md5(f"{lang_field}:{terms_str}".encode()).hexdigest()[:16]
        return f"{lang_field}{mode_suffix}_{len(glossary_terms)}_{hash_value}"

    def get(
        self,
        glossary_terms: List[Dict],
        lang_field: str,
        normalize_mode: bool = False
    ) -> Optional[Any]:
        """ìºì‹œì—ì„œ automaton ì¡°íšŒ"""
        key = self._generate_cache_key(glossary_terms, lang_field, normalize_mode)
        if key in self._cache:
            # LRU: ì ‘ê·¼ ìˆœì„œ ì—…ë°ì´íŠ¸
            if key in self._access_order:
                self._access_order.remove(key)
            self._access_order.append(key)
            logger.debug(f"ğŸ¯ Automaton ìºì‹œ íˆíŠ¸: {key}")
            return self._cache[key]
        return None

    def set(
        self,
        glossary_terms: List[Dict],
        lang_field: str,
        automaton: Any,
        normalize_mode: bool = False
    ) -> None:
        """ìºì‹œì— automaton ì €ì¥"""
        key = self._generate_cache_key(glossary_terms, lang_field, normalize_mode)

        # LRU: ìºì‹œ í¬ê¸° ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self._cache) >= self._max_size and key not in self._cache:
            oldest_key = self._access_order.pop(0)
            del self._cache[oldest_key]
            logger.debug(f"ğŸ—‘ï¸ Automaton ìºì‹œ ì œê±° (LRU): {oldest_key}")

        self._cache[key] = automaton
        if key in self._access_order:
            self._access_order.remove(key)
        self._access_order.append(key)
        logger.debug(f"ğŸ’¾ Automaton ìºì‹œ ì €ì¥: {key}")

    def clear(self) -> None:
        """ìºì‹œ ì „ì²´ ì´ˆê¸°í™”"""
        self._cache.clear()
        self._access_order.clear()
        logger.info("ğŸ§¹ Automaton ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")


# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_automaton_cache = AutomatonCache(max_size=50)


class OptimizedTermDetectorAgent(BaseAgent):
    """
    Aho-Corasick ê¸°ë°˜ ê³ ì„±ëŠ¥ ìš©ì–´ íƒì§€ Agent

    ì±…ì„: í…ìŠ¤íŠ¸ + ìš©ì–´ì§‘ â†’ íƒì§€ëœ ìš©ì–´ (ìœ„ì¹˜ í¬í•¨)

    ì‹œê°„ë³µì¡ë„: O(M + Z)
    - M: í…ìŠ¤íŠ¸ ê¸¸ì´
    - Z: ì´ ë§¤ì¹­ ìˆ˜

    ê¸°ì¡´ TermDetectorAgent ëŒ€ë¹„ ì•½ 1,000~10,000ë°° ì„±ëŠ¥ í–¥ìƒ

    ì˜ˆì‹œ:
        >>> agent = OptimizedTermDetectorAgent()
        >>> glossary = [
        ...     {"korean_term": "ì¸ê³µì§€ëŠ¥", "english_term": "AI"},
        ...     {"korean_term": "ë¨¸ì‹ ëŸ¬ë‹", "english_term": "Machine Learning"}
        ... ]
        >>> text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ì¤‘ìš”í•©ë‹ˆë‹¤"
        >>> detected = await agent.process(text, glossary)
        >>> len(detected)
        2

    Note:
        ì´ AgentëŠ” OpenAI APIë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìˆœìˆ˜ ë¬¸ìì—´ ë§¤ì¹­ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        BaseAgentë¥¼ ìƒì†ë°›ì§€ë§Œ self.clientëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """

    # ì–¸ì–´ë³„ ì •ê·œí™” ì ìš© ì—¬ë¶€ ì„¤ì •
    # í•œêµ­ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´: ë„ì–´ì“°ê¸° ê·œì¹™ì´ ìœ ì—°í•˜ë¯€ë¡œ ì •ê·œí™” ì ìš©
    # ì˜ì–´/ë² íŠ¸ë‚¨ì–´: ë„ì–´ì“°ê¸°ê°€ ë‹¨ì–´ êµ¬ë¶„ì´ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ì •ê·œí™” ë¹„ì ìš©
    NORMALIZE_LANGUAGES = {"ko", "ja", "zh"}

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        super().__init__()
        self._cache = _automaton_cache

    def _get_lang_field(self, source_lang: str) -> str:
        """
        source_langì— ë”°ë¼ ë§¤ì¹­í•  ìš©ì–´ì§‘ í•„ë“œëª… ë°˜í™˜

        Args:
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ko, en, vi ë“±)

        Returns:
            ìš©ì–´ì§‘ í•„ë“œëª… (korean_term, english_term, vietnamese_term)
        """
        lang_field_map = {
            "ko": "korean_term",
            "en": "english_term",
            "vi": "vietnamese_term",
            "ja": "korean_term",  # ì¼ë³¸ì–´ëŠ” í•œê¸€ë¡œ fallback
            "zh": "korean_term",  # ì¤‘êµ­ì–´ëŠ” í•œê¸€ë¡œ fallback
        }
        return lang_field_map.get(source_lang, "korean_term")

    def _create_position_mapping(
        self,
        text: str,
        normalize_chars: str = " \t\n"
    ) -> PositionMapping:
        """
        í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ìœ„ì¹˜ ë§¤í•‘ í…Œì´ë¸” ìƒì„±

        ê³µë°± ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ ,
        ì •ê·œí™”ëœ ì¸ë±ìŠ¤ë¥¼ ì›ë³¸ ì¸ë±ìŠ¤ë¡œ ì—­ì‚°í•  ìˆ˜ ìˆëŠ” ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            normalize_chars: ì œê±°í•  ë¬¸ìë“¤ (ê¸°ë³¸: ê³µë°±ë¥˜)

        Returns:
            PositionMapping: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ì™€ ë§¤í•‘ ì •ë³´

        Example:
            >>> text = "ì¸ê³µ ì§€ëŠ¥ê³¼ ë¨¸ì‹  ëŸ¬ë‹"
            >>> mapping = self._create_position_mapping(text)
            >>> mapping.normalized_text
            'ì¸ê³µì§€ëŠ¥ê³¼ë¨¸ì‹ ëŸ¬ë‹'
            >>> mapping.norm_to_orig
            [0, 1, 3, 4, 5, 7, 8, 10, 11]

        Time Complexity: O(M) where M = len(text)
        """
        normalized_chars: List[str] = []
        norm_to_orig: List[int] = []

        for orig_idx, char in enumerate(text):
            if char not in normalize_chars:
                normalized_chars.append(char)
                norm_to_orig.append(orig_idx)

        return PositionMapping(
            normalized_text=''.join(normalized_chars),
            original_text=text,
            norm_to_orig=norm_to_orig
        )

    def _normalize_term(self, term: str) -> str:
        """
        ìš©ì–´ì—ì„œ ê³µë°± ì œê±°

        Args:
            term: ìš©ì–´ ë¬¸ìì—´

        Returns:
            ê³µë°±ì´ ì œê±°ëœ ìš©ì–´

        Example:
            >>> self._normalize_term("ì¸ê³µ ì§€ëŠ¥")
            'ì¸ê³µì§€ëŠ¥'
        """
        return ''.join(term.split())

    def _map_to_original_position(
        self,
        norm_start: int,
        norm_end: int,
        mapping: PositionMapping
    ) -> Tuple[int, int]:
        """
        ì •ê·œí™”ëœ ìœ„ì¹˜ë¥¼ ì›ë³¸ ìœ„ì¹˜ë¡œ ë³€í™˜

        Args:
            norm_start: ì •ê·œí™” í…ìŠ¤íŠ¸ì—ì„œì˜ ì‹œì‘ ìœ„ì¹˜
            norm_end: ì •ê·œí™” í…ìŠ¤íŠ¸ì—ì„œì˜ ì¢…ë£Œ ìœ„ì¹˜ (exclusive)
            mapping: ìœ„ì¹˜ ë§¤í•‘ ì •ë³´

        Returns:
            (ì›ë³¸ ì‹œì‘ ìœ„ì¹˜, ì›ë³¸ ì¢…ë£Œ ìœ„ì¹˜)

        Example:
            >>> # ì›ë³¸: "ì¸ê³µ ì§€ëŠ¥ê³¼" â†’ ì •ê·œí™”: "ì¸ê³µì§€ëŠ¥ê³¼"
            >>> # "ì¸ê³µì§€ëŠ¥" ë§¤ì¹­ ì‹œ norm_start=0, norm_end=4
            >>> orig_start, orig_end = self._map_to_original_position(0, 4, mapping)
            >>> orig_start, orig_end
            (0, 5)  # ì›ë³¸ì—ì„œ "ì¸ê³µ ì§€ëŠ¥"ì˜ ë²”ìœ„
        """
        orig_start = mapping.norm_to_orig[norm_start]
        orig_end = mapping.norm_to_orig[norm_end - 1] + 1

        return orig_start, orig_end

    def _build_automaton(
        self,
        glossary_terms: List[Dict],
        lang_field: str,
        normalize_mode: bool = False
    ) -> Any:
        """
        Aho-Corasick automaton êµ¬ì¶•

        Args:
            glossary_terms: ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸
            lang_field: ë§¤ì¹­í•  ì–¸ì–´ í•„ë“œ (korean_term, english_term, vietnamese_term)
            normalize_mode: ì •ê·œí™” ëª¨ë“œ ì—¬ë¶€ (Trueë©´ ìš©ì–´ì—ì„œ ê³µë°± ì œê±°)

        Returns:
            êµ¬ì¶•ëœ Aho-Corasick automaton

        Note:
            ê¸´ ìš©ì–´ë¥¼ ë¨¼ì € ì¶”ê°€í•˜ì—¬ ê¸´ ìš©ì–´ ìš°ì„  ë§¤ì¹­ì„ ë³´ì¥í•©ë‹ˆë‹¤.
            normalize_mode=Trueì¼ ë•ŒëŠ” ì •ê·œí™”ëœ ìš©ì–´(ê³µë°± ì œê±°)ë¡œ automatonì„ ë¹Œë“œí•©ë‹ˆë‹¤.
        """
        if not AHOCORASICK_AVAILABLE:
            raise RuntimeError("pyahocorasick ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        A = ahocorasick.Automaton()

        # ê¸´ ìš©ì–´ ìš°ì„  ì •ë ¬ (ê¸´ ìš©ì–´ê°€ ì§§ì€ ìš©ì–´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        # ì •ê·œí™” ëª¨ë“œì—ì„œëŠ” ì •ê·œí™”ëœ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        def get_term_length(t: Dict) -> int:
            term = t.get(lang_field, "") or ""
            if normalize_mode:
                return len(self._normalize_term(term))
            return len(term)

        sorted_terms = sorted(
            glossary_terms,
            key=get_term_length,
            reverse=True
        )

        for term_dict in sorted_terms:
            search_term = term_dict.get(lang_field)
            if search_term and search_term.strip():
                # ì •ê·œí™” ëª¨ë“œ: ê³µë°± ì œê±°
                if normalize_mode:
                    normalized_term = self._normalize_term(search_term)
                    if normalized_term:
                        # ì†Œë¬¸ìë¡œ ì €ì¥ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ìš©)
                        # value: (ì •ê·œí™”ëœ í…ìŠ¤íŠ¸, ì „ì²´ ìš©ì–´ ì •ë³´)
                        A.add_word(
                            normalized_term.lower(),
                            (normalized_term, term_dict)
                        )
                else:
                    # ì¼ë°˜ ëª¨ë“œ: ì›ë³¸ ê·¸ëŒ€ë¡œ
                    A.add_word(
                        search_term.lower(),
                        (search_term, term_dict)
                    )

        A.make_automaton()

        mode_str = "ì •ê·œí™”" if normalize_mode else "ì¼ë°˜"
        logger.debug(f"ğŸ”§ Automaton ë¹Œë“œ ì™„ë£Œ ({mode_str} ëª¨ë“œ): {len(glossary_terms)}ê°œ ìš©ì–´")
        return A

    def _get_or_build_automaton(
        self,
        glossary_terms: List[Dict],
        lang_field: str,
        normalize_mode: bool = False
    ) -> Any:
        """
        ìºì‹œì—ì„œ automaton ì¡°íšŒ ë˜ëŠ” ìƒˆë¡œ ë¹Œë“œ

        Args:
            glossary_terms: ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸
            lang_field: ë§¤ì¹­í•  ì–¸ì–´ í•„ë“œ
            normalize_mode: ì •ê·œí™” ëª¨ë“œ ì—¬ë¶€

        Returns:
            Aho-Corasick automaton
        """
        # ìºì‹œ í™•ì¸ (ì •ê·œí™” ëª¨ë“œë³„ ë³„ë„ ìºì‹œ)
        cached = self._cache.get(glossary_terms, lang_field, normalize_mode)
        if cached is not None:
            return cached

        # ìºì‹œ ë¯¸ìŠ¤: ìƒˆë¡œ ë¹Œë“œ
        automaton = self._build_automaton(glossary_terms, lang_field, normalize_mode)
        self._cache.set(glossary_terms, lang_field, automaton, normalize_mode)

        return automaton

    async def process(
        self,
        text: str,
        glossary_terms: List[Dict[str, str]],
        source_lang: str = "ko",
        case_sensitive: bool = False,
        normalize_whitespace: bool = True
    ) -> List[DetectedTerm]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ì§‘ì˜ ìš©ì–´ë¥¼ íƒì§€ (Aho-Corasick ì•Œê³ ë¦¬ì¦˜)

        ì‹œê°„ë³µì¡ë„: O(M + Z)
        - M: í…ìŠ¤íŠ¸ ê¸¸ì´
        - Z: ì´ ë§¤ì¹­ ìˆ˜

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            glossary_terms: ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
                ê° ë”•ì…”ë„ˆë¦¬ëŠ” ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•¨:
                - korean_term (í•„ìˆ˜): í•œê¸€ ìš©ì–´
                - english_term (ì„ íƒ): ì˜ì–´ ìš©ì–´
                - vietnamese_term (ì„ íƒ): ë² íŠ¸ë‚¨ì–´ ìš©ì–´
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ko, en, vi ë“±) - í•´ë‹¹ ì–¸ì–´ ìš©ì–´ ë§¤ì¹­
            case_sensitive: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            normalize_whitespace: ë„ì–´ì“°ê¸° ì •ê·œí™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                - True: "ì¸ê³µ ì§€ëŠ¥" == "ì¸ê³µì§€ëŠ¥" ìœ¼ë¡œ ë§¤ì¹­ (í•œêµ­ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´)
                - False: ì •í™•í•œ ë¬¸ìì—´ë§Œ ë§¤ì¹­

        Returns:
            íƒì§€ëœ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ìœ„ì¹˜ ì •ë³´ í¬í•¨, ìœ„ì¹˜ìˆœ ì •ë ¬)

        Raises:
            ValueError: í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
            RuntimeError: pyahocorasickì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°

        Example:
            >>> agent = OptimizedTermDetectorAgent()
            >>> text = "í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì»¨í…Œì´ë„ˆë¥¼ ë°°í¬í•©ë‹ˆë‹¤"
            >>> glossary = [
            ...     {"korean_term": "í´ë¼ìš°ë“œ", "english_term": "Cloud"},
            ...     {"korean_term": "ì»¨í…Œì´ë„ˆ", "english_term": "Container"}
            ... ]
            >>> detected = await agent.process(text, glossary, source_lang="ko")
            >>> for term in detected:
            ...     print(f"{term.matched_text} at [{term.position_start}:{term.position_end}]")
            í´ë¼ìš°ë“œ at [0:4]
            ì»¨í…Œì´ë„ˆ at [10:14]

            # ë„ì–´ì“°ê¸° ì •ê·œí™” ì˜ˆì‹œ
            >>> glossary = [{"korean_term": "ì¸ê³µ ì§€ëŠ¥", "english_term": "AI"}]
            >>> text = "ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•©ë‹ˆë‹¤"  # ë„ì–´ì“°ê¸° ì—†ìŒ
            >>> detected = await agent.process(text, glossary, source_lang="ko")
            >>> detected[0].matched_text
            'ì¸ê³µì§€ëŠ¥'  # ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€
        """
        if not text or not text.strip():
            raise ValueError("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        if not glossary_terms:
            logger.debug("ìš©ì–´ì§‘ì´ ë¹„ì–´ìˆì–´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []

        if not AHOCORASICK_AVAILABLE:
            logger.warning("âš ï¸ pyahocorasick ë¯¸ì„¤ì¹˜. ê¸°ì¡´ regex ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            return await self._fallback_regex_process(
                text, glossary_terms, source_lang, case_sensitive, normalize_whitespace
            )

        # ì–¸ì–´ë³„ ì •ê·œí™” ì ìš© ì—¬ë¶€ ê²°ì •
        # í•œêµ­ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´: ë„ì–´ì“°ê¸° ê·œì¹™ì´ ìœ ì—°í•˜ë¯€ë¡œ ì •ê·œí™” ì ìš©
        # ì˜ì–´/ë² íŠ¸ë‚¨ì–´: ë„ì–´ì“°ê¸°ê°€ ë‹¨ì–´ êµ¬ë¶„ì´ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ì •ê·œí™” ë¹„ì ìš©
        should_normalize = normalize_whitespace and source_lang in self.NORMALIZE_LANGUAGES

        if should_normalize:
            return await self._process_with_normalization(
                text, glossary_terms, source_lang, case_sensitive
            )
        else:
            return await self._process_exact_match(
                text, glossary_terms, source_lang, case_sensitive
            )

    async def _process_exact_match(
        self,
        text: str,
        glossary_terms: List[Dict[str, str]],
        source_lang: str,
        case_sensitive: bool
    ) -> List[DetectedTerm]:
        """
        ì •í™• ë§¤ì¹­ ëª¨ë“œ (ê¸°ì¡´ ë¡œì§)

        ë„ì–´ì“°ê¸° ì •ê·œí™” ì—†ì´ ì •í™•í•œ ë¬¸ìì—´ ë§¤ì¹­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        lang_field = self._get_lang_field(source_lang)

        logger.info(f"ğŸ” Aho-Corasick ìš©ì–´ íƒì§€ ì‹œì‘ (ì •í™• ë§¤ì¹­): {len(glossary_terms)}ê°œ ìš©ì–´")

        # Automaton ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ë˜ëŠ” ë¹Œë“œ)
        automaton = self._get_or_build_automaton(glossary_terms, lang_field, normalize_mode=False)

        # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ (ëŒ€ì†Œë¬¸ì ì²˜ë¦¬)
        search_text = text if case_sensitive else text.lower()

        # O(M) ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ ëª¨ë“  ìš©ì–´ íƒì§€
        detected_terms: List[DetectedTerm] = []
        matched_positions = set()  # ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€

        for end_pos, (original_term, term_dict) in automaton.iter(search_text):
            term_len = len(original_term)
            start_pos = end_pos - term_len + 1

            # ì´ë¯¸ ë§¤ì¹­ëœ ìœ„ì¹˜ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (ê¸´ ìš©ì–´ ìš°ì„ )
            current_range = range(start_pos, end_pos + 1)
            if any(pos in matched_positions for pos in current_range):
                continue

            # í•œê¸€ ìš©ì–´ì˜ ê²½ìš°: ì•ì— í•œê¸€ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ (ë‹¨ì–´ ê²½ê³„ ì²´í¬)
            if not original_term.isascii():
                if start_pos > 0:
                    prev_char = text[start_pos - 1]
                    # ì• ë¬¸ìê°€ í•œê¸€ì´ë©´ ìŠ¤í‚µ
                    if '\uac00' <= prev_char <= '\ud7a3':
                        continue

            # ë§¤ì¹­ëœ ìœ„ì¹˜ ê¸°ë¡
            matched_positions.update(current_range)

            # ì›ë¬¸ì—ì„œ ì‹¤ì œ ë§¤ì¹­ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ì›ë³¸ ìœ ì§€)
            actual_matched_text = text[start_pos:end_pos + 1]

            detected_terms.append(DetectedTerm(
                matched_text=actual_matched_text,
                position_start=start_pos,
                position_end=end_pos + 1,  # ê¸°ì¡´ APIì™€ í˜¸í™˜ (exclusive end)
                korean_term=term_dict.get("korean_term"),
                english_term=term_dict.get("english_term"),
                vietnamese_term=term_dict.get("vietnamese_term")
            ))

        # ìœ„ì¹˜ìˆœìœ¼ë¡œ ì •ë ¬
        detected_terms.sort(key=lambda t: t.position_start)

        logger.info(f"âœ… ìš©ì–´ íƒì§€ ì™„ë£Œ: {len(detected_terms)}ê°œ íƒì§€ (ì •í™• ë§¤ì¹­)")

        return detected_terms

    async def _process_with_normalization(
        self,
        text: str,
        glossary_terms: List[Dict[str, str]],
        source_lang: str,
        case_sensitive: bool
    ) -> List[DetectedTerm]:
        """
        ì •ê·œí™” ê¸°ë°˜ ìš©ì–´ íƒì§€

        ë„ì–´ì“°ê¸°ë¥¼ ë¬´ì‹œí•˜ê³  ìš©ì–´ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
        "ì¸ê³µ ì§€ëŠ¥" â†” "ì¸ê³µì§€ëŠ¥" ê°™ì€ ë³€í˜•ì„ ëª¨ë‘ ë§¤ì¹­í•©ë‹ˆë‹¤.

        ì‹œê°„ë³µì¡ë„: O(M + Z) ìœ ì§€ (ì •ê·œí™”ì— O(M) ì¶”ê°€)

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            glossary_terms: ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ
            case_sensitive: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—¬ë¶€

        Returns:
            íƒì§€ëœ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€ ìœ„ì¹˜ ì •ë³´ í¬í•¨)
        """
        lang_field = self._get_lang_field(source_lang)

        logger.info(f"ğŸ” Aho-Corasick ìš©ì–´ íƒì§€ ì‹œì‘ (ì •ê·œí™” ëª¨ë“œ): {len(glossary_terms)}ê°œ ìš©ì–´")

        # Step 1: í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        mapping = self._create_position_mapping(text)
        logger.debug(f"ğŸ“ í…ìŠ¤íŠ¸ ì •ê·œí™”: {len(text)}ì â†’ {len(mapping.normalized_text)}ì")

        if not mapping.normalized_text:
            logger.debug("ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []

        # Step 2: ì •ê·œí™”ëœ ìš©ì–´ë¡œ Automaton ë¹Œë“œ
        automaton = self._get_or_build_automaton(glossary_terms, lang_field, normalize_mode=True)

        # Step 3: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰
        search_text = mapping.normalized_text if case_sensitive else mapping.normalized_text.lower()

        # Step 4: O(M) ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ ëª¨ë“  ìš©ì–´ íƒì§€
        detected_terms: List[DetectedTerm] = []
        matched_positions = set()  # ì •ê·œí™”ëœ ìœ„ì¹˜ ê¸°ì¤€ ì¤‘ë³µ ë°©ì§€

        for end_pos, (normalized_term, term_dict) in automaton.iter(search_text):
            term_len = len(normalized_term)
            norm_start = end_pos - term_len + 1
            norm_end = end_pos + 1  # exclusive

            # ì´ë¯¸ ë§¤ì¹­ëœ ìœ„ì¹˜ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (ê¸´ ìš©ì–´ ìš°ì„ )
            current_range = range(norm_start, norm_end)
            if any(pos in matched_positions for pos in current_range):
                continue

            # Step 5: ì›ë³¸ ìœ„ì¹˜ë¡œ ì—­ì‚° (ë‹¨ì–´ ê²½ê³„ ì²´í¬ë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰)
            orig_start, orig_end = self._map_to_original_position(
                norm_start, norm_end, mapping
            )

            # í•œê¸€ ìš©ì–´ì˜ ê²½ìš°: ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì•ì— í•œê¸€ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ (ë‹¨ì–´ ê²½ê³„ ì²´í¬)
            # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì²´í¬í•´ì•¼ í•¨
            # (ì •ê·œí™” ì‹œ ê³µë°±ì´ ì œê±°ë˜ì–´ ì›ë˜ ë¶„ë¦¬ëœ ë‹¨ì–´ê°€ ë¶™ì–´ë²„ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
            if not normalized_term.isascii():
                if orig_start > 0:
                    prev_char = text[orig_start - 1]
                    # ì• ë¬¸ìê°€ í•œê¸€ì´ë©´ ìŠ¤í‚µ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë“±ì€ OK)
                    if '\uac00' <= prev_char <= '\ud7a3':
                        continue

            # ë§¤ì¹­ëœ ìœ„ì¹˜ ê¸°ë¡ (ì •ê·œí™”ëœ ìœ„ì¹˜)
            matched_positions.update(current_range)

            # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë§¤ì¹­ëœ ë¬¸ìì—´ ì¶”ì¶œ
            actual_matched_text = text[orig_start:orig_end]

            detected_terms.append(DetectedTerm(
                matched_text=actual_matched_text,
                position_start=orig_start,
                position_end=orig_end,
                korean_term=term_dict.get("korean_term"),
                english_term=term_dict.get("english_term"),
                vietnamese_term=term_dict.get("vietnamese_term")
            ))

        # ìœ„ì¹˜ìˆœìœ¼ë¡œ ì •ë ¬
        detected_terms.sort(key=lambda t: t.position_start)

        logger.info(f"âœ… ìš©ì–´ íƒì§€ ì™„ë£Œ: {len(detected_terms)}ê°œ íƒì§€ (ì •ê·œí™” ëª¨ë“œ)")

        return detected_terms

    async def _fallback_regex_process(
        self,
        text: str,
        glossary_terms: List[Dict[str, str]],
        source_lang: str,
        case_sensitive: bool,
        normalize_whitespace: bool = True
    ) -> List[DetectedTerm]:
        """
        Fallback: ê¸°ì¡´ regex ê¸°ë°˜ íƒì§€ (pyahocorasick ë¯¸ì„¤ì¹˜ ì‹œ)

        ì„±ëŠ¥ì´ ë‚®ìœ¼ë¯€ë¡œ ê°€ëŠ¥í•˜ë©´ pyahocorasick ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

        Args:
            text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
            glossary_terms: ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸
            source_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ
            case_sensitive: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—¬ë¶€
            normalize_whitespace: ë„ì–´ì“°ê¸° ì •ê·œí™” ì—¬ë¶€

        Returns:
            íƒì§€ëœ ìš©ì–´ ë¦¬ìŠ¤íŠ¸
        """
        import re

        primary_field = self._get_lang_field(source_lang)

        # ì •ê·œí™” ëª¨ë“œ ê²°ì •
        should_normalize = normalize_whitespace and source_lang in self.NORMALIZE_LANGUAGES

        detected_terms: List[DetectedTerm] = []
        matched_positions = set()

        # ì •ê·œí™” ëª¨ë“œì¼ ë•Œ ë§¤í•‘ ìƒì„±
        mapping: Optional[PositionMapping] = None
        search_text = text
        if should_normalize:
            mapping = self._create_position_mapping(text)
            search_text = mapping.normalized_text

        # ê¸´ ìš©ì–´ë¶€í„° ë§¤ì¹­
        def get_term_length(t: Dict) -> int:
            term = t.get(primary_field, "") or ""
            if should_normalize:
                return len(self._normalize_term(term))
            return len(term)

        sorted_terms = sorted(
            glossary_terms,
            key=get_term_length,
            reverse=True
        )

        for term_dict in sorted_terms:
            search_term = term_dict.get(primary_field)
            if not search_term:
                continue

            # ì •ê·œí™” ëª¨ë“œì¼ ë•Œ ìš©ì–´ë„ ì •ê·œí™”
            if should_normalize:
                search_term = self._normalize_term(search_term)
                if not search_term:
                    continue

            escaped_term = re.escape(search_term)

            # ì •ê·œí™” ëª¨ë“œì—ì„œëŠ” ë‹¨ì–´ ê²½ê³„ ì²´í¬ë¥¼ ë‚˜ì¤‘ì— ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰
            if should_normalize:
                # ì •ê·œí™” ëª¨ë“œ: ë‹¨ì–´ ê²½ê³„ ì—†ì´ ë§¤ì¹­ (ë‚˜ì¤‘ì— ì›ë³¸ì—ì„œ ì²´í¬)
                pattern = escaped_term
            elif search_term.isascii():
                pattern = r'\b' + escaped_term + r'\b'
            else:
                pattern = r'(?<![ê°€-í£])' + escaped_term

            flags = re.IGNORECASE if not case_sensitive else 0

            for match in re.finditer(pattern, search_text, flags):
                start, end = match.span()

                if any(pos in range(start, end) for pos in matched_positions):
                    continue

                # ì •ê·œí™” ëª¨ë“œì¼ ë•Œ ì›ë³¸ ìœ„ì¹˜ë¡œ ì—­ì‚°
                if should_normalize and mapping:
                    orig_start, orig_end = self._map_to_original_position(
                        start, end, mapping
                    )

                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ê²½ê³„ ì²´í¬ (í•œê¸€ ìš©ì–´)
                    if not search_term.isascii():
                        if orig_start > 0:
                            prev_char = text[orig_start - 1]
                            # ì• ë¬¸ìê°€ í•œê¸€ì´ë©´ ìŠ¤í‚µ
                            if '\uac00' <= prev_char <= '\ud7a3':
                                continue

                    actual_matched_text = text[orig_start:orig_end]
                else:
                    orig_start, orig_end = start, end
                    actual_matched_text = match.group()

                matched_positions.update(range(start, end))

                detected_terms.append(DetectedTerm(
                    matched_text=actual_matched_text,
                    position_start=orig_start,
                    position_end=orig_end,
                    korean_term=term_dict.get("korean_term"),
                    english_term=term_dict.get("english_term"),
                    vietnamese_term=term_dict.get("vietnamese_term")
                ))

        detected_terms.sort(key=lambda t: t.position_start)

        mode_str = "ì •ê·œí™”" if should_normalize else "ì •í™•"
        logger.warning(f"âš ï¸ Regex fallback ì‚¬ìš© ({mode_str} ëª¨ë“œ): {len(detected_terms)}ê°œ íƒì§€")

        return detected_terms

    @staticmethod
    def clear_cache() -> None:
        """
        ì „ì—­ Automaton ìºì‹œ ì´ˆê¸°í™”

        ìš©ì–´ì§‘ì´ ëŒ€ëŸ‰ìœ¼ë¡œ ë³€ê²½ëœ ê²½ìš° í˜¸ì¶œí•˜ì—¬
        ìºì‹œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        _automaton_cache.clear()
