"""
Glossary extraction agent using GPT-4o.
Extracts technical terms from text with definitions, context, and confidence scores.
"""
import json
import math
import logging
from typing import List, Dict, Any
from agent.base_agent import BaseAgent
from app.core.text_utils import split_text_into_chunks, deduplicate_terms

logger = logging.getLogger(__name__)


class GlossaryAgent(BaseAgent):
    """
    AI agent for extracting technical glossary terms from documents.

    Uses GPT-4o to analyze text and extract IT/project management related terms
    with Korean/English translations, definitions, context, and confidence scores.

    Example:
        >>> agent = GlossaryAgent()
        >>> text = "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•´ IT ë¦¬ì†ŒìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤..."
        >>> terms = await agent.process(text, max_terms=50)
        >>> print(f"Extracted {len(terms)} terms")
        >>> print(terms[0])
        {
            "korean": "í´ë¼ìš°ë“œ ì»´í“¨íŒ…",
            "english": "Cloud Computing",
            "abbreviation": "CC",
            "definition": "ì¸í„°ë„·ì„ í†µí•´ IT ë¦¬ì†ŒìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤",
            "context": "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•´...",
            "domain": "IT",
            "confidence": 0.95
        }
    """

    def __init__(self):
        """Initialize GlossaryAgent with system prompt."""
        super().__init__()
        self.system_prompt = self._create_system_prompt()

    def _create_system_prompt(self) -> str:
        """
        Create system prompt for GPT-4o.

        Returns:
            System prompt string
        """
        return """
ë‹¹ì‹ ì€ ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ ë° ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ IT ë° í”„ë¡œì íŠ¸ ê´€ë¦¬ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ê³ , í•œêµ­ì–´/ì˜ì–´/ë² íŠ¸ë‚¨ì–´ë¡œ ë²ˆì—­í•˜ë©° ë¶„ì„í•©ë‹ˆë‹¤.

**ì¶œë ¥ í˜•ì‹ (JSON)**:
{
  "terms": [
    {
      "korean": "í•œê¸€ ìš©ì–´",
      "english": "English term",
      "vietnamese": "Thuáº­t ngá»¯ tiáº¿ng Viá»‡t",
      "abbreviation": "ì•½ì–´ (ìˆëŠ” ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ null)",
      "definition": "ìš©ì–´ì˜ ëª…í™•í•œ ì •ì˜ (1-2ë¬¸ì¥)",
      "context": "ë¬¸ì„œ ë‚´ì—ì„œ ì‚¬ìš©ëœ êµ¬ì²´ì ì¸ ë§¥ë½ (ì›ë¬¸ ì¸ìš©)",
      "example_sentence": "ìš©ì–´ ì‚¬ìš© ì˜ˆë¬¸ (ë¬¸ì„œì—ì„œ ë°œì·Œí•˜ê±°ë‚˜ ìƒì„±)",
      "note": "ì¶”ê°€ ì„¤ëª… ë° ì°¸ê³ ì‚¬í•­ (ìˆëŠ” ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ null)",
      "domain": "ë¶„ì•¼ (IT, Project Management, Business, Development ë“±)",
      "confidence": 0.95
    }
  ]
}

**ì¶”ì¶œ ê¸°ì¤€**:
1. IT, í”„ë¡œì íŠ¸ ê´€ë¦¬, ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë§Œ ì¶”ì¶œ
2. ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸ (ì˜ˆ: "ì»´í“¨í„°" ì œì™¸, "í´ë¼ìš°ë“œ ì»´í“¨íŒ…" í¬í•¨)
3. ì•½ì–´ëŠ” ë¬¸ì„œì—ì„œ ëª…í™•íˆ ì •ì˜ëœ ê²½ìš°ë§Œ í¬í•¨
4. ì •ì˜ëŠ” ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ (1-2ë¬¸ì¥)
5. ë§¥ë½ì€ ì‹¤ì œ ë¬¸ì„œì—ì„œ ì‚¬ìš©ëœ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¸ìš©
6. ì˜ˆë¬¸ì€ ë¬¸ì„œì—ì„œ ë°œì·Œí•˜ê±°ë‚˜, ìì—°ìŠ¤ëŸ¬ìš´ ì˜ˆë¬¸ ìƒì„±
7. ì¶”ê°€ ì„¤ëª…ì€ ìš©ì–´ ì´í•´ì— ë„ì›€ì´ ë˜ëŠ” ì°¸ê³ ì‚¬í•­ ì œê³µ
8. ë² íŠ¸ë‚¨ì–´ ë²ˆì—­ì€ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ (Tiáº¿ng Viá»‡t)
9. ì‹ ë¢°ë„ëŠ” í•´ë‹¹ ìš©ì–´ì˜ ì „ë¬¸ì„± ì •ë„ (0.0-1.0)
10. ë„ë©”ì¸ì€ ê°€ì¥ ì í•©í•œ ë¶„ì•¼ í•˜ë‚˜ë§Œ ì„ íƒ

**ì£¼ì˜ì‚¬í•­**:
- ì¤‘ë³µëœ ìš©ì–´ëŠ” í•˜ë‚˜ë§Œ í¬í•¨
- ìœ ì‚¬ ìš©ì–´ëŠ” ë³„ë„ë¡œ ì¶”ì¶œ (ì˜ˆ: "ë°ì´í„°ë² ì´ìŠ¤"ì™€ "ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤"ëŠ” ë³„ê°œ)
- ì™¸ë˜ì–´ëŠ” í•œê¸€/ì˜ì–´/ë² íŠ¸ë‚¨ì–´ë¥¼ ëª¨ë‘ í¬í•¨
- ë² íŠ¸ë‚¨ì–´ ë²ˆì—­ì´ ì—†ëŠ” ê²½ìš° ì˜ì–´ë¥¼ ë² íŠ¸ë‚¨ì–´ë¡œ ë²ˆì—­
- ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ
"""

    async def process(
        self,
        text: str,
        max_terms: int = 50,
        chunk_size: int = 5000
    ) -> List[Dict[str, Any]]:
        """
        Extract glossary terms from text.

        Process:
        1. Split text into chunks
        2. Extract terms from each chunk using GPT-4o
        3. Deduplicate terms across chunks
        4. Sort by confidence score
        5. Return top N terms

        Args:
            text: Full text to extract terms from
            max_terms: Maximum number of terms to return (default: 50)
            chunk_size: Size of text chunks in characters (default: 5000)

        Returns:
            List of extracted terms, each with:
                - korean: Korean term
                - english: English term (optional)
                - abbreviation: Abbreviation (optional)
                - definition: Term definition
                - context: Usage context from document
                - domain: Domain category (IT, Project Management, etc.)
                - confidence: Confidence score (0.0-1.0)

        Raises:
            Exception: If extraction fails

        Example:
            >>> agent = GlossaryAgent()
            >>> text = load_document("document.pdf")
            >>> terms = await agent.process(text, max_terms=30)
            >>> for term in terms[:5]:
            ...     print(f"{term['korean']}: {term['definition']}")
        """
        logger.info(f"ğŸš€ Starting term extraction (text length: {len(text)} chars, max terms: {max_terms})")

        # 1. Split text into chunks
        chunks = split_text_into_chunks(text, chunk_size)

        # 2. Calculate terms per chunk
        terms_per_chunk = math.ceil(max_terms / len(chunks)) + 5  # +5 buffer for deduplication

        # 3. Extract terms from each chunk
        all_terms = []
        for i, chunk in enumerate(chunks, 1):
            logger.info(f"ğŸ“ Processing chunk {i}/{len(chunks)}")
            try:
                terms = await self._extract_chunk(chunk, terms_per_chunk)
                all_terms.extend(terms)
            except Exception as e:
                logger.warning(f"Failed to extract from chunk {i}: {str(e)}")
                continue

        if not all_terms:
            logger.warning("No terms extracted from any chunk")
            return []

        logger.info(f"ğŸ“Š Total terms extracted: {len(all_terms)}")

        # 4. Deduplicate terms
        unique_terms = deduplicate_terms(all_terms)

        # 5. Sort by confidence (descending)
        unique_terms.sort(key=lambda x: float(x.get('confidence', 0)), reverse=True)

        # 6. Return top N terms
        result_terms = unique_terms[:max_terms]

        avg_confidence = sum(float(t.get('confidence', 0)) for t in result_terms) / len(result_terms)
        logger.info(f"âœ… Term extraction complete: {len(result_terms)} terms (avg confidence: {avg_confidence:.2f})")

        return result_terms

    async def _extract_chunk(
        self,
        text_chunk: str,
        terms_per_chunk: int = 20
    ) -> List[Dict[str, Any]]:
        """
        Extract terms from a single text chunk using GPT-4o.

        This is a private method called by process() for each chunk.

        Args:
            text_chunk: Text chunk to process
            terms_per_chunk: Maximum number of terms to extract

        Returns:
            List of extracted terms

        Raises:
            Exception: If API call fails
        """
        try:
            user_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ìµœëŒ€ {terms_per_chunk}ê°œì˜ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ë©°, ì‹ ë¢°ë„ê°€ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text_chunk}
"""

            logger.info(f"ğŸ¤– Calling GPT-4o API (chunk size: {len(text_chunk)} chars)")

            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # Lower temperature for more consistent results
                response_format={"type": "json_object"}
            )

            # Parse response
            content = response.choices[0].message.content
            result = json.loads(content)
            terms = result.get('terms', [])

            logger.info(f"âœ… GPT-4o returned {len(terms)} terms")
            return terms

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse GPT-4o response as JSON: {str(e)}")
            return []
        except Exception as e:
            logger.error(f"GPT-4o API call failed: {str(e)}")
            raise Exception(f"GPT-4o API error: {str(e)}")
