"""
Video Translation Service

ì˜ìƒ ìë§‰ STT ë° ë²ˆì—­ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Java Backend V22 ìŠ¤í‚¤ë§ˆì™€ í˜¸í™˜ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤ (files/video_files ì‹œìŠ¤í…œ).
"""

import logging
import math
import os
from typing import List, Dict, Any, Optional
from uuid import UUID
from pathlib import Path
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified

# Agent imports
from agent.video.stt_agent import VideoSTTAgent
from agent.video.subtitle_generator_agent import SubtitleGeneratorAgent
from agent.translate.context_enhanced_translation_agent import ContextEnhancedTranslationAgent
from agent.term_detection.term_detector_agent import TermDetectorAgent

# Model imports
from app.models.file import File  # For context documents
from app.models.video_file import VideoFile
from app.models.video_subtitle import VideoSubtitle

# Config
from app.config import settings

logger = logging.getLogger(__name__)


class VideoTranslationService:
    """
    ì˜ìƒ ë²ˆì—­ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§

    ì±…ì„:
    - STT Agentë¥¼ í†µí•œ ìŒì„± ì¸ì‹
    - ë²ˆì—­ Agentë¥¼ í†µí•œ ìë§‰ ë²ˆì—­
    - ìë§‰ íŒŒì¼ ìƒì„± ë° DB ì €ì¥
    """

    def __init__(self):
        """Agent ì¸ìŠ¤í„´ìŠ¤í™”"""
        self.stt_agent = VideoSTTAgent()
        self.subtitle_generator = SubtitleGeneratorAgent()
        self.context_translator = ContextEnhancedTranslationAgent()
        self.term_detector = TermDetectorAgent()

    def _get_video_file_by_file_id(self, file_id: UUID, db: Session) -> VideoFile:
        """
        íŒŒì¼ IDë¡œ ì˜ìƒ íŒŒì¼ ì¡°íšŒ

        Args:
            file_id: íŒŒì¼ ID (File)
            db: DB ì„¸ì…˜

        Returns:
            VideoFile ê°ì²´

        Raises:
            ValueError: ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
        """
        video_file = db.query(VideoFile).filter(
            VideoFile.id == file_id
        ).first()

        if not video_file:
            raise ValueError(f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")

        return video_file

    def _get_file_by_id(self, file_id: UUID, db: Session) -> File:
        """
        íŒŒì¼ IDë¡œ íŒŒì¼ ì¡°íšŒ

        Args:
            file_id: íŒŒì¼ ID
            db: DB ì„¸ì…˜

        Returns:
            File ê°ì²´

        Raises:
            ValueError: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
        """
        file = db.query(File).filter(File.id == file_id).first()

        if not file:
            raise ValueError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")

        return file

    def _fetch_project_glossary(
        self,
        project_id: UUID,
        db: Session
    ) -> List[Dict[str, Any]]:
        """
        í”„ë¡œì íŠ¸ì˜ ìš©ì–´ì§‘ ì¡°íšŒ (translation_service.pyì™€ ë™ì¼í•œ ë¡œì§)

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            db: DB ì„¸ì…˜

        Returns:
            ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)

        Note:
            project_files ì¡°ì¸ì„ í†µí•´ í”„ë¡œì íŠ¸ì— ì—°ê²°ëœ ëª¨ë“  ë¬¸ì„œì˜ ìš©ì–´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        """
        from sqlalchemy import text

        # Native SQL ì¿¼ë¦¬: Javaì˜ findTermsByProjectFiles()ì™€ ë™ì¼í•œ ë¡œì§
        query = text("""
            SELECT DISTINCT t.*
            FROM glossary_terms t
            INNER JOIN glossary_term_documents gtd ON t.id = gtd.term_id
            INNER JOIN project_files pf ON gtd.file_id = pf.file_id
            WHERE pf.project_id = :project_id
        """)

        result = db.execute(query, {"project_id": str(project_id)})
        rows = result.fetchall()

        # Rowë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        glossary_terms = []
        for row in rows:
            glossary_terms.append({
                "id": str(row.id),
                "korean_term": row.korean_term,
                "english_term": row.english_term,
                "vietnamese_term": row.vietnamese_term,
                "definition": row.definition,
                "context": row.context,
                "example_sentence": row.example_sentence,
                "note": row.note,
                "domain": row.domain,
                "confidence_score": float(row.confidence_score) if row.confidence_score else None
            })

        logger.info(f"ğŸ“š í”„ë¡œì íŠ¸ ìš©ì–´ì§‘ ì¡°íšŒ: {len(glossary_terms)}ê°œ ìš©ì–´")
        return glossary_terms

    def _fetch_project_documents_text(
        self,
        project_id: UUID,
        db: Session,
        max_docs: int = 5
    ) -> str:
        """
        í”„ë¡œì íŠ¸ì˜ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¡°íšŒ (ì»¨í…ìŠ¤íŠ¸ ìƒì„±ìš©)

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            db: DB ì„¸ì…˜
            max_docs: ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 5ê°œ)

        Returns:
            í†µí•©ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸
        """
        # í”„ë¡œì íŠ¸ì™€ ì—°ê²°ëœ ë¬¸ì„œ ì¡°íšŒ
        documents = db.query(File).join(
            File.projects
        ).filter(
            File.projects.any(id=project_id)
        ).limit(max_docs).all()

        document_texts = []
        for doc in documents:
            if doc.contents:
                full_text = "\n".join([
                    content.content_text
                    for content in doc.contents
                    if content.content_text
                ])
                if full_text:
                    document_texts.append(full_text[:2000])  # ê° ë¬¸ì„œ 2000ì ì œí•œ

        combined_text = "\n\n".join(document_texts)
        logger.info(f"ğŸ“„ í”„ë¡œì íŠ¸ ë¬¸ì„œ ì¡°íšŒ: {len(documents)}ê°œ, ì´ {len(combined_text)}ì")

        return combined_text

    def _fetch_context_documents_text(
        self,
        document_ids: List[UUID],
        db: Session
    ) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¡°íšŒ

        Args:
            document_ids: ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            db: DB ì„¸ì…˜

        Returns:
            í†µí•©ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸
        """
        if not document_ids:
            return ""

        documents = db.query(File).filter(File.id.in_(document_ids)).all()

        document_texts = []
        for doc in documents:
            if doc.contents:
                full_text = "\n".join([
                    content.content_text
                    for content in doc.contents
                    if content.content_text
                ])
                if full_text:
                    document_texts.append(full_text[:2000])  # ê° ë¬¸ì„œ 2000ì ì œí•œ

        combined_text = "\n\n".join(document_texts)
        logger.info(f"ğŸ“„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¡°íšŒ: {len(documents)}ê°œ, ì´ {len(combined_text)}ì")

        return combined_text

    async def process_stt(
        self,
        video_file_id: UUID,
        source_language: str,
        user_id: UUID,
        db: Session
    ) -> Dict[str, Any]:
        """
        ì˜ìƒ STT ì²˜ë¦¬ ë° DB ì €ì¥

        Args:
            video_file_id: ì˜ìƒ íŒŒì¼ ID (File ID, not VideoFile ID)
            source_language: ìŒì„± ì–¸ì–´ ì½”ë“œ
            user_id: ì‚¬ìš©ì ID
            db: DB ì„¸ì…˜

        Returns:
            STT ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ¥ STT ì²˜ë¦¬ ì‹œì‘: file={video_file_id}, lang={source_language}")

        # Step 1: File ë° VideoFile ì¡°íšŒ
        file = self._get_file_by_id(video_file_id, db)
        video_file = self._get_video_file_by_file_id(video_file_id, db)

        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í™˜ê²½ì— ë”°ë¼ ìë™ ê°ì§€)
        upload_base_dir = settings.upload_dir
        full_video_path = os.path.join(upload_base_dir, file.file_path)

        if not os.path.exists(full_video_path):
            raise FileNotFoundError(f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_video_path}")

        # Step 2: STT ì²˜ë¦¬ (Agent í˜¸ì¶œ)
        segments = await self.stt_agent.process(
            video_file_path=full_video_path,
            source_language=source_language
        )

        logger.info(f"ğŸ¤ STT ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

        # Step 3: ê¸°ì¡´ ìë§‰ ì‚­ì œ (ì¬ì²˜ë¦¬ í—ˆìš©)
        deleted_count = db.query(VideoSubtitle).filter(
            VideoSubtitle.video_file_id == video_file.id
        ).delete()

        if deleted_count > 0:
            logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ìë§‰ ì‚­ì œ: {deleted_count}ê°œ")

        # Step 4: DB ì €ì¥ (ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ VideoSubtitle rowë¡œ ì €ì¥)
        for segment_data in segments:
            # Whisper avg_logprobì€ ìŒìˆ˜ê°’ì´ë¯€ë¡œ exp()ë¡œ 0-1 í™•ë¥ ë¡œ ë³€í™˜
            raw_confidence = segment_data.get("confidence")
            if raw_confidence is not None and raw_confidence < 0:
                # avg_logprob â†’ probability: exp(logprob)
                confidence = math.exp(raw_confidence)
                # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                confidence = max(0.0, min(1.0, confidence))
            else:
                confidence = raw_confidence

            subtitle = VideoSubtitle(
                video_file_id=video_file.id,
                sequence_number=segment_data["sequence_number"],
                start_time_ms=segment_data["start_time_ms"],
                end_time_ms=segment_data["end_time_ms"],
                original_text=segment_data["text"],
                original_language=source_language,  # ì›ë³¸ ì–¸ì–´ ì €ì¥
                translations={},  # ë¹ˆ ë²ˆì—­ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
                translated_text=None,  # ë ˆê±°ì‹œ í•„ë“œ (í•˜ìœ„ í˜¸í™˜ì„±)
                confidence_score=confidence
            )
            db.add(subtitle)

        db.commit()

        # Step 5: ì €ì¥ëœ ìë§‰ ì¡°íšŒ
        saved_subtitles = db.query(VideoSubtitle).filter(
            VideoSubtitle.video_file_id == video_file.id
        ).order_by(VideoSubtitle.sequence_number).all()

        logger.info(f"âœ… STT ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(saved_subtitles)}ê°œ ìë§‰")

        # ì‘ë‹µ êµ¬ì„±
        return {
            "video_file_id": video_file_id,
            "language": source_language,
            "segments": [
                {
                    "sequence_number": sub.sequence_number,
                    "start_time_ms": sub.start_time_ms,
                    "end_time_ms": sub.end_time_ms,
                    "text": sub.original_text,
                    "confidence": float(sub.confidence_score) if sub.confidence_score else None
                }
                for sub in saved_subtitles
            ],
            "total_segments": len(saved_subtitles),
            "created_at": saved_subtitles[0].created_at if saved_subtitles else None
        }

    async def process_translation(
        self,
        video_file_id: UUID,
        project_id: Optional[UUID],  # Text.vue ë°©ì‹: projectIdë¡œ ìš©ì–´ì§‘ ìë™ ì¡°íšŒ
        source_language: str,
        target_language: str,
        user_id: UUID,
        db: Session
    ) -> Dict[str, Any]:
        """
        ì˜ìƒ ìë§‰ ë²ˆì—­ ë° DB ì €ì¥ (Text.vue ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ projectIdë¡œ ìš©ì–´ì§‘ ì¡°íšŒ)

        Args:
            video_file_id: ì˜ìƒ íŒŒì¼ ID (File ID)
            project_id: í”„ë¡œì íŠ¸ ID (ìš©ì–´ì§‘ ì»¨í…ìŠ¤íŠ¸ ìë™ ì¡°íšŒ) - Noneì´ë©´ ê¸°ë³¸ ë²ˆì—­
            source_language: ì›ë³¸ ì–¸ì–´
            target_language: ëª©í‘œ ì–¸ì–´
            user_id: ì‚¬ìš©ì ID
            db: DB ì„¸ì…˜

        Returns:
            ë²ˆì—­ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸŒ ìë§‰ ë²ˆì—­ ì‹œì‘: {source_language} â†’ {target_language}, project={project_id}")

        # Step 1: VideoFile ì¡°íšŒ
        video_file = self._get_video_file_by_file_id(video_file_id, db)

        # Step 2: ì›ë³¸ ìë§‰ ì¡°íšŒ
        original_subtitles = db.query(VideoSubtitle).filter(
            VideoSubtitle.video_file_id == video_file.id
        ).order_by(VideoSubtitle.sequence_number).all()

        if not original_subtitles:
            raise ValueError(
                f"ì›ë³¸ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. STTë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”. "
                f"(file={video_file_id})"
            )

        logger.info(f"ğŸ“ ì›ë³¸ ìë§‰ ì¡°íšŒ: {len(original_subtitles)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

        # Step 3: í”„ë¡œì íŠ¸ ê¸°ë°˜ ìš©ì–´ì§‘ ë° ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (Text.vue ë°©ì‹)
        glossary_terms = []
        context_text = ""
        context_used = False

        if project_id:
            # í”„ë¡œì íŠ¸ ìš©ì–´ì§‘ ì¡°íšŒ
            glossary_terms = self._fetch_project_glossary(project_id, db)
            logger.info(f"ğŸ“š ìš©ì–´ì§‘ ì¡°íšŒ: {len(glossary_terms)}ê°œ ìš©ì–´")

            # í”„ë¡œì íŠ¸ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
            context_text = self._fetch_project_documents_text(project_id, db)
            context_used = len(context_text) > 0 or len(glossary_terms) > 0

        # Step 4: ê° ì„¸ê·¸ë¨¼íŠ¸ ë²ˆì—­ (ContextEnhancedTranslationAgent ì‚¬ìš©)
        # ìš©ì–´ì§‘ì„ ë²ˆì—­ìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        glossary_for_translation = [
            {
                "korean_term": t["korean_term"],
                "english_term": t.get("english_term"),
                "vietnamese_term": t.get("vietnamese_term")
            }
            for t in glossary_terms
        ]

        total_detected_count = 0
        for subtitle in original_subtitles:
            # ìš©ì–´ íƒì§€ (TermDetectorAgent ì‚¬ìš©)
            detected_terms = []
            if glossary_terms:
                detected_terms = await self.term_detector.process(
                    text=subtitle.original_text,
                    glossary_terms=glossary_for_translation,
                    source_lang=source_language
                )
                total_detected_count += len(detected_terms)

                # íƒì§€ëœ ìš©ì–´ë¥¼ DBì— ì €ì¥ (JSONB í˜•ì‹)
                if detected_terms:
                    subtitle.detected_terms = [
                        {
                            "matched_text": t.matched_text,
                            "korean_term": t.korean_term,
                            "english_term": t.english_term,
                            "vietnamese_term": t.vietnamese_term
                        }
                        for t in detected_terms
                    ]
                    flag_modified(subtitle, "detected_terms")

            # ë²ˆì—­ (ì»¨í…ìŠ¤íŠ¸ + ìš©ì–´ì§‘ + íƒì§€ëœ ìš©ì–´ í¬í•¨)
            translated_text = await self.context_translator.process(
                text=subtitle.original_text,
                source_lang=source_language,
                target_lang=target_language,
                context=context_text if context_used else "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ",
                glossary_terms=glossary_for_translation,
                detected_terms=detected_terms
            )

            # DB ì—…ë°ì´íŠ¸ (translations JSONì— target_language ì¶”ê°€)
            if subtitle.translations is None:
                subtitle.translations = {}

            subtitle.translations[target_language] = translated_text
            flag_modified(subtitle, "translations")  # JSONB ë³€ê²½ ì¶”ì 

            # ë ˆê±°ì‹œ í•„ë“œë„ ì—…ë°ì´íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)
            subtitle.translated_text = translated_text

        db.commit()

        logger.info(
            f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(original_subtitles)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ "
            f"(ìš©ì–´ì§‘: {len(glossary_terms)}ê°œ, íƒì§€ëœ ìš©ì–´: {total_detected_count}ê°œ)"
        )

        # ì‘ë‹µ êµ¬ì„±
        return {
            "video_file_id": video_file_id,
            "source_language": source_language,
            "target_language": target_language,
            "segments": [
                {
                    "sequence_number": sub.sequence_number,
                    "start_time_ms": sub.start_time_ms,
                    "end_time_ms": sub.end_time_ms,
                    "original_text": sub.original_text,
                    "translated_text": sub.translated_text,
                    "confidence": float(sub.confidence_score) if sub.confidence_score else None
                }
                for sub in original_subtitles
            ],
            "total_segments": len(original_subtitles),
            "context_used": context_used,
            "context_document_count": len(glossary_terms),  # ìš©ì–´ ìˆ˜ë¡œ ë³€ê²½
            "created_at": original_subtitles[0].created_at if original_subtitles else None
        }

    async def get_subtitles(
        self,
        video_file_id: UUID,
        db: Session
    ) -> Dict[str, Any]:
        """
        ë‹¤êµ­ì–´ ìë§‰ ì¡°íšŒ

        Args:
            video_file_id: ì˜ìƒ íŒŒì¼ ID (File ID)
            db: DB ì„¸ì…˜

        Returns:
            ë‹¤êµ­ì–´ ìë§‰ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ“– ë‹¤êµ­ì–´ ìë§‰ ì¡°íšŒ: video_file_id={video_file_id}")

        # Step 1: VideoFile ì¡°íšŒ
        video_file = self._get_video_file_by_file_id(video_file_id, db)

        # Step 2: ìë§‰ ì¡°íšŒ
        subtitles = db.query(VideoSubtitle).filter(
            VideoSubtitle.video_file_id == video_file.id
        ).order_by(VideoSubtitle.sequence_number).all()

        if not subtitles:
            raise ValueError(f"ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_file_id}")

        # Step 3: ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ ëª©ë¡ ì¶”ì¶œ
        original_language = subtitles[0].original_language if subtitles else "ko"
        available_languages = {original_language}  # ì›ë³¸ ì–¸ì–´ëŠ” í•­ìƒ í¬í•¨

        # ëª¨ë“  ìë§‰ì˜ translationsì—ì„œ ì–¸ì–´ ì¶”ì¶œ
        for subtitle in subtitles:
            if subtitle.translations:
                available_languages.update(subtitle.translations.keys())

        logger.info(f"âœ… ìë§‰ ì¡°íšŒ ì™„ë£Œ: {len(subtitles)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {len(available_languages)}ê°œ ì–¸ì–´")

        # ì‘ë‹µ êµ¬ì„±
        return {
            "video_file_id": video_file_id,
            "original_language": original_language,
            "available_languages": sorted(list(available_languages)),
            "segments": [
                {
                    "sequence_number": sub.sequence_number,
                    "start_time_ms": sub.start_time_ms,
                    "end_time_ms": sub.end_time_ms,
                    "original_text": sub.original_text,
                    "translations": sub.translations or {},
                    "confidence": float(sub.confidence_score) if sub.confidence_score else None,
                    "detected_terms": sub.detected_terms or []
                }
                for sub in subtitles
            ],
            "total_segments": len(subtitles)
        }

    async def generate_subtitle_file(
        self,
        video_file_id: UUID,
        language_type: str,  # "original" or "translated"
        db: Session
    ) -> Dict[str, Any]:
        """
        SRT ìë§‰ íŒŒì¼ ìƒì„±

        Args:
            video_file_id: ì˜ìƒ íŒŒì¼ ID (File ID)
            language_type: "original" (ì›ë³¸) or "translated" (ë²ˆì—­)
            db: DB ì„¸ì…˜

        Returns:
            íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ“„ SRT íŒŒì¼ ìƒì„± ì‹œì‘: video_file_id={video_file_id}, type={language_type}")

        # Step 1: VideoFile ì¡°íšŒ
        video_file = self._get_video_file_by_file_id(video_file_id, db)

        # Step 2: ìë§‰ ì¡°íšŒ
        subtitles = db.query(VideoSubtitle).filter(
            VideoSubtitle.video_file_id == video_file.id
        ).order_by(VideoSubtitle.sequence_number).all()

        if not subtitles:
            raise ValueError("ìë§‰ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        # Step 3: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„± (í™˜ê²½ì— ë”°ë¼ ìë™ ê°ì§€)
        subtitle_dir = Path(settings.upload_dir) / "subtitles"
        subtitle_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{video_file.id}_{language_type}.srt"
        output_path = subtitle_dir / filename

        # Step 4: SubtitleGeneratorAgentë¡œ SRT íŒŒì¼ ìƒì„±
        segments_data = []
        for sub in subtitles:
            text = sub.translated_text if language_type == "translated" else sub.original_text
            if text:  # ë²ˆì—­ ì „ì´ë©´ translated_textê°€ Noneì¼ ìˆ˜ ìˆìŒ
                segments_data.append({
                    "sequence_number": sub.sequence_number,
                    "start_time_ms": sub.start_time_ms,
                    "end_time_ms": sub.end_time_ms,
                    "text": text
                })

        file_path = await self.subtitle_generator.process(
            segments=segments_data,
            output_path=str(output_path),
            subtitle_type=language_type.upper()
        )

        logger.info(f"âœ… SRT íŒŒì¼ ìƒì„± ì™„ë£Œ: {file_path}")

        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(file_path)

        return {
            "video_file_id": video_file_id,
            "file_path": file_path,
            "language_type": language_type,
            "file_size_bytes": file_size
        }
