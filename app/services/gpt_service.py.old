"""
GPT-4o service for glossary term extraction.
Uses OpenAI API to extract technical terms from text.
"""
import logging
import json
import math
from typing import List, Dict, Any
from openai import AsyncOpenAI
from app.config import settings

logger = logging.getLogger(__name__)

# Initialize OpenAI client
client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

# System prompt for GPT-4o
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ IT ë° í”„ë¡œì íŠ¸ ê´€ë¦¬ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

**ì¶œë ¥ í˜•ì‹ (JSON)**:
{
  "terms": [
    {
      "korean": "í•œê¸€ ìš©ì–´",
      "english": "English term",
      "abbreviation": "ì•½ì–´ (ìˆëŠ” ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ null)",
      "definition": "ìš©ì–´ì˜ ëª…í™•í•œ ì •ì˜ (1-2ë¬¸ì¥)",
      "context": "ë¬¸ì„œ ë‚´ì—ì„œ ì‚¬ìš©ëœ êµ¬ì²´ì ì¸ ë§¥ë½ (ì›ë¬¸ ì¸ìš©)",
      "domain": "ë¶„ì•¼ (IT, Project Management, Business, Development ë“±)",
      "confidence": 0.95
    }
  ]
}

**ì¶”ì¶œ ê¸°ì¤€**:
1. IT, í”„ë¡œì íŠ¸ ê´€ë¦¬, ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë§Œ ì¶”ì¶œ
2. ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸ (ì˜ˆ: "ì»´í“¨í„°" ì œì™¸, "í´ë¼ìš°ë“œ ì»´í“¨íŒ…" í¬í•¨)
3. ì•½ì–´ëŠ” ë¬¸ì„œì—ì„œ ëª…í™•íˆ ì •ì˜ëœ ê²½ìš°ë§Œ í¬í•¨
4. ì •ì˜ëŠ” ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ (1-2ë¬¸ì¥)
5. ë§¥ë½ì€ ì‹¤ì œ ë¬¸ì„œì—ì„œ ì‚¬ìš©ëœ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¸ìš©
6. ì‹ ë¢°ë„ëŠ” í•´ë‹¹ ìš©ì–´ì˜ ì „ë¬¸ì„± ì •ë„ (0.0-1.0)
7. ë„ë©”ì¸ì€ ê°€ì¥ ì í•©í•œ ë¶„ì•¼ í•˜ë‚˜ë§Œ ì„ íƒ

**ì£¼ì˜ì‚¬í•­**:
- ì¤‘ë³µëœ ìš©ì–´ëŠ” í•˜ë‚˜ë§Œ í¬í•¨
- ìœ ì‚¬ ìš©ì–´ëŠ” ë³„ë„ë¡œ ì¶”ì¶œ (ì˜ˆ: "ë°ì´í„°ë² ì´ìŠ¤"ì™€ "ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤"ëŠ” ë³„ê°œ)
- ì™¸ë˜ì–´ëŠ” ê°€ëŠ¥í•œ í•œê¸€ê³¼ ì˜ì–´ë¥¼ ëª¨ë‘ í¬í•¨
- ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ
"""


def split_text_into_chunks(text: str, chunk_size: int = 5000) -> List[str]:
    """
    Split text into chunks of specified size.

    Args:
        text: Text to split
        chunk_size: Maximum size of each chunk (default: 5000 characters)

    Returns:
        List of text chunks
    """
    chunks = []
    words = text.split()
    current_chunk = []
    current_size = 0

    for word in words:
        word_size = len(word) + 1  # +1 for space
        if current_size + word_size > chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = [word]
            current_size = word_size
        else:
            current_chunk.append(word)
            current_size += word_size

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    logger.info(f"ğŸ“Š Text split into {len(chunks)} chunks (avg size: {sum(len(c) for c in chunks) // len(chunks)} chars)")
    return chunks


def deduplicate_terms(terms: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove duplicate terms based on korean term.
    If duplicates exist, keep the one with highest confidence.

    Args:
        terms: List of term dictionaries

    Returns:
        List of unique terms
    """
    unique_terms = {}

    for term in terms:
        korean = term.get('korean', '').strip()
        if not korean:
            continue

        if korean not in unique_terms:
            unique_terms[korean] = term
        else:
            # Keep term with higher confidence
            existing_confidence = float(unique_terms[korean].get('confidence', 0))
            new_confidence = float(term.get('confidence', 0))
            if new_confidence > existing_confidence:
                unique_terms[korean] = term

    logger.info(f"ğŸ” Deduplicated: {len(terms)} â†’ {len(unique_terms)} unique terms")
    return list(unique_terms.values())


async def extract_terms_from_chunk(
    text_chunk: str,
    terms_per_chunk: int = 20
) -> List[Dict[str, Any]]:
    """
    Extract terms from a single text chunk using GPT-4o.

    Args:
        text_chunk: Text chunk to process
        terms_per_chunk: Maximum number of terms to extract

    Returns:
        List of extracted terms

    Raises:
        Exception: If API call fails
    """
    try:
        user_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ìµœëŒ€ {terms_per_chunk}ê°œì˜ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ë©°, ì‹ ë¢°ë„ê°€ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text_chunk}
"""

        logger.info(f"ğŸ¤– Calling GPT-4o API (chunk size: {len(text_chunk)} chars)")

        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,  # Lower temperature for more consistent results
            response_format={"type": "json_object"}
        )

        # Parse response
        content = response.choices[0].message.content
        result = json.loads(content)
        terms = result.get('terms', [])

        logger.info(f"âœ… GPT-4o returned {len(terms)} terms")
        return terms

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse GPT-4o response as JSON: {str(e)}")
        return []
    except Exception as e:
        logger.error(f"GPT-4o API call failed: {str(e)}")
        raise Exception(f"GPT-4o API error: {str(e)}")


async def extract_terms_with_gpt(
    text: str,
    max_terms: int = 50,
    chunk_size: int = 5000
) -> List[Dict[str, Any]]:
    """
    Extract glossary terms from text using GPT-4o.

    Process:
    1. Split text into chunks
    2. Extract terms from each chunk
    3. Deduplicate terms
    4. Sort by confidence
    5. Return top N terms

    Args:
        text: Full text to extract terms from
        max_terms: Maximum number of terms to return (default: 50)
        chunk_size: Size of text chunks (default: 5000 chars)

    Returns:
        List of extracted terms (dict with korean, english, abbreviation, definition, context, domain, confidence)

    Raises:
        Exception: If extraction fails
    """
    logger.info(f"ğŸš€ Starting term extraction (text length: {len(text)} chars, max terms: {max_terms})")

    # Split text into chunks
    chunks = split_text_into_chunks(text, chunk_size)

    # Calculate terms per chunk
    terms_per_chunk = math.ceil(max_terms / len(chunks)) + 5  # +5 buffer for deduplication

    # Extract terms from each chunk
    all_terms = []
    for i, chunk in enumerate(chunks, 1):
        logger.info(f"ğŸ“ Processing chunk {i}/{len(chunks)}")
        try:
            terms = await extract_terms_from_chunk(chunk, terms_per_chunk)
            all_terms.extend(terms)
        except Exception as e:
            logger.warning(f"Failed to extract from chunk {i}: {str(e)}")
            continue

    if not all_terms:
        logger.warning("No terms extracted from any chunk")
        return []

    logger.info(f"ğŸ“Š Total terms extracted: {len(all_terms)}")

    # Deduplicate terms
    unique_terms = deduplicate_terms(all_terms)

    # Sort by confidence (descending)
    unique_terms.sort(key=lambda x: float(x.get('confidence', 0)), reverse=True)

    # Return top N terms
    result_terms = unique_terms[:max_terms]

    logger.info(f"âœ… Term extraction complete: {len(result_terms)} terms (avg confidence: {sum(float(t.get('confidence', 0)) for t in result_terms) / len(result_terms):.2f})")

    return result_terms
