"""
ë²ˆì—­ Service

ì—¬ëŸ¬ Micro Agentë¥¼ ì¡°ìœ¨í•˜ì—¬ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ì„ ì œê³µí•©ë‹ˆë‹¤.

ì„±ëŠ¥ ìµœì í™”:
- OptimizedTermDetectorAgent: Aho-Corasick ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ O(M+Z) ìš©ì–´ íƒì§€
- GlossaryCache: í”„ë¡œì íŠ¸ë³„ ìš©ì–´ì§‘ TTL ìºì‹±
"""

import logging
from typing import Optional, Dict, List, Any
from uuid import UUID
from sqlalchemy.orm import Session

# Agent imports
from agent.translate.simple_translation_agent import SimpleTranslationAgent
from agent.translate.context_enhanced_translation_agent import ContextEnhancedTranslationAgent
from agent.term_detection.optimized_term_detector_agent import OptimizedTermDetectorAgent
from agent.term_detection.glossary_matcher_agent import GlossaryMatcherAgent
from agent.summarization.document_summarizer_agent import DocumentSummarizerAgent

# Core imports
from app.core.glossary_cache import glossary_cache

# Model imports
from app.models.translation import Translation, TranslationTerm
from app.models.glossary import GlossaryTerm
from app.models.file import File

logger = logging.getLogger(__name__)


class TranslationService:
    """
    ë²ˆì—­ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§

    ì±…ì„:
    - Micro Agentë“¤ì„ ì¡°ìœ¨í•˜ì—¬ ë²ˆì—­ ìˆ˜í–‰
    - í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë° ìš©ì–´ì§‘ ë°ì´í„° ì¡°íšŒ
    - ë²ˆì—­ ê²°ê³¼ë¥¼ DBì— ì €ì¥
    - íƒì§€ëœ ìš©ì–´ ë§¤í•‘ ê´€ë¦¬

    ì„±ëŠ¥ ìµœì í™”:
    - OptimizedTermDetectorAgent: Aho-Corasick ê¸°ë°˜ O(M+Z) ìš©ì–´ íƒì§€
    - GlossaryCache: í”„ë¡œì íŠ¸ë³„ ìš©ì–´ì§‘ ìºì‹± (TTL 5ë¶„)
    """

    def __init__(self):
        """Micro Agent ì¸ìŠ¤í„´ìŠ¤í™”"""
        self.simple_translator = SimpleTranslationAgent()
        self.context_translator = ContextEnhancedTranslationAgent()
        # ìµœì í™”ëœ ìš©ì–´ íƒì§€ Agent ì‚¬ìš© (Aho-Corasick ì•Œê³ ë¦¬ì¦˜)
        self.term_detector = OptimizedTermDetectorAgent()
        self.glossary_matcher = GlossaryMatcherAgent()
        self.document_summarizer = DocumentSummarizerAgent()
        # ìš©ì–´ì§‘ ìºì‹œ ì°¸ì¡°
        self._glossary_cache = glossary_cache

    def _fetch_project_glossary(
        self,
        project_id: UUID,
        db: Session,
        use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """
        í”„ë¡œì íŠ¸ì˜ ìš©ì–´ì§‘ ì¡°íšŒ (ìºì‹± ì ìš©)

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            db: DB ì„¸ì…˜
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

        Returns:
            ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)

        Note:
            - ìºì‹œ TTL: 5ë¶„ (glossary_cache ê¸°ë³¸ê°’)
            - project_id = NULLì¸ ìš©ì–´ë„ ì¡°íšŒí•˜ê¸° ìœ„í•´
              glossary_term_documents â†’ project_files ê²½ë¡œë¡œ ì¡°ì¸í•©ë‹ˆë‹¤.

        Performance:
            - ìºì‹œ íˆíŠ¸ ì‹œ: O(1)
            - ìºì‹œ ë¯¸ìŠ¤ ì‹œ: DB ì¿¼ë¦¬ ìˆ˜í–‰
        """
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached = self._glossary_cache.get(project_id)
            if cached is not None:
                logger.debug(f"ğŸ“¦ ìš©ì–´ì§‘ ìºì‹œ íˆíŠ¸: project={project_id}, terms={len(cached)}ê°œ")
                return cached

        from sqlalchemy import text

        # Native SQL ì¿¼ë¦¬: Javaì˜ findTermsByProjectFiles()ì™€ ë™ì¼í•œ ë¡œì§
        query = text("""
            SELECT DISTINCT t.*
            FROM glossary_terms t
            INNER JOIN glossary_term_documents gtd ON t.id = gtd.term_id
            INNER JOIN project_files pf ON gtd.file_id = pf.file_id
            WHERE pf.project_id = :project_id
        """)

        result = db.execute(query, {"project_id": str(project_id)})
        rows = result.fetchall()

        # Rowë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        glossary_terms = []
        for row in rows:
            glossary_terms.append({
                "id": row.id,
                "korean_term": row.korean_term,
                "english_term": row.english_term,
                "vietnamese_term": row.vietnamese_term,
                "definition": row.definition,
                "context": row.context,
                "example_sentence": row.example_sentence,
                "note": row.note,
                "domain": row.domain,
                "confidence_score": float(row.confidence_score) if row.confidence_score else None
            })

        # ìºì‹œ ì €ì¥
        if use_cache and glossary_terms:
            self._glossary_cache.set(project_id, glossary_terms)
            logger.debug(f"ğŸ’¾ ìš©ì–´ì§‘ ìºì‹œ ì €ì¥: project={project_id}, terms={len(glossary_terms)}ê°œ")

        return glossary_terms

    def _fetch_project_documents_text(
        self,
        project_id: UUID,
        db: Session,
        max_docs: int = 5
    ) -> List[str]:
        """
        í”„ë¡œì íŠ¸ì˜ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¡°íšŒ

        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            db: DB ì„¸ì…˜
            max_docs: ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 5ê°œ)

        Returns:
            ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        # í”„ë¡œì íŠ¸ì™€ ì—°ê²°ëœ ë¬¸ì„œ ì¡°íšŒ
        documents = db.query(File).join(
            File.projects
        ).filter(
            File.projects.any(id=project_id)
        ).limit(max_docs).all()

        document_texts = []
        for doc in documents:
            # DocumentContentì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if doc.contents:
                # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹¨
                full_text = "\n".join([
                    content.content_text
                    for content in doc.contents
                    if content.content_text
                ])
                if full_text:
                    # ê¸¸ì´ ì œí•œ (ê° ë¬¸ì„œë‹¹ 2000ì)
                    document_texts.append(full_text[:2000])

        return document_texts

    async def translate_text(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        user_id: UUID,
        project_id: Optional[UUID],
        db: Session
    ) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ë²ˆì—­ (í”„ë¡œì íŠ¸ ì„ íƒ ì—¬ë¶€ì— ë”°ë¼ Agent ì¡°í•© ë³€ê²½)

        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            source_lang: ì›ë³¸ ì–¸ì–´
            target_lang: ëª©í‘œ ì–¸ì–´
            user_id: ì‚¬ìš©ì ID
            project_id: í”„ë¡œì íŠ¸ ID (Noneì´ë©´ ê¸°ë³¸ ë²ˆì—­)
            db: DB ì„¸ì…˜

        Returns:
            ë²ˆì—­ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸŒ ë²ˆì—­ ìš”ì²­: user={user_id}, project={project_id}, {source_lang}â†’{target_lang}")

        # Case 1: í”„ë¡œì íŠ¸ ì—†ìŒ â†’ SimpleTranslationAgentë§Œ ì‚¬ìš©
        if project_id is None:
            logger.info("ğŸ“ ê¸°ë³¸ ë²ˆì—­ ëª¨ë“œ (í”„ë¡œì íŠ¸ ì—†ìŒ)")

            translated_text = await self.simple_translator.process(
                text, source_lang, target_lang
            )

            # DB ì €ì¥
            translation = Translation(
                user_id=user_id,
                project_id=None,
                original_text=text,
                translated_text=translated_text,
                source_language=source_lang,
                target_language=target_lang,
                context_used=False,
                context_summary=None,
                terms_detected=0
            )
            db.add(translation)
            db.commit()
            db.refresh(translation)

            return {
                "translation_id": str(translation.id),
                "original_text": text,
                "translated_text": translated_text,
                "source_language": source_lang,
                "target_language": target_lang,
                "context_used": False,
                "context_summary": None,
                "detected_terms": [],
                "terms_count": 0
            }

        # Case 2: í”„ë¡œì íŠ¸ ìˆìŒ â†’ Multiple Agents ì¡°í•©
        logger.info(f"ğŸš€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­ ëª¨ë“œ (í”„ë¡œì íŠ¸: {project_id})")

        # Step 1: í”„ë¡œì íŠ¸ì˜ ìš©ì–´ì§‘ ì¡°íšŒ
        glossary_terms = self._fetch_project_glossary(project_id, db)
        logger.info(f"ğŸ“š ìš©ì–´ì§‘ ì¡°íšŒ: {len(glossary_terms)}ê°œ")

        # Step 2: ìš©ì–´ íƒì§€ (source_langì— ë”°ë¼ í•´ë‹¹ ì–¸ì–´ ìš©ì–´ ë§¤ì¹­)
        detected_terms = await self.term_detector.process(
            text,
            [{"korean_term": t["korean_term"],
              "english_term": t.get("english_term"),
              "vietnamese_term": t.get("vietnamese_term")}
             for t in glossary_terms],
            source_lang=source_lang
        )
        logger.info(f"ğŸ” ìš©ì–´ íƒì§€: {len(detected_terms)}ê°œ (source_lang={source_lang})")

        # Step 3: ìš©ì–´ ë§¤ì¹­ (ìƒì„¸ ì •ë³´)
        matched_terms = await self.glossary_matcher.process(detected_terms, glossary_terms)
        logger.info(f"âœ… ìš©ì–´ ë§¤ì¹­: {len(matched_terms)}ê°œ")

        # Step 4: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        project_documents = self._fetch_project_documents_text(project_id, db)
        context_summary = ""
        if project_documents:
            logger.info(f"ğŸ“„ ë¬¸ì„œ ì¡°íšŒ: {len(project_documents)}ê°œ")
            context_summary = await self.document_summarizer.process(
                project_documents,
                max_length=500
            )
            logger.info(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ì™„ë£Œ: {len(context_summary)}ì")
        else:
            logger.warning("âš ï¸ í”„ë¡œì íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")

        # Step 5: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²ˆì—­
        translated_text = await self.context_translator.process(
            text=text,
            source_lang=source_lang,
            target_lang=target_lang,
            context=context_summary or "í”„ë¡œì íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤",
            glossary_terms=[{"korean_term": t["korean_term"],
                             "english_term": t.get("english_term"),
                             "vietnamese_term": t.get("vietnamese_term")}
                            for t in glossary_terms],
            detected_terms=detected_terms
        )

        # Step 6: DB ì €ì¥
        translation = Translation(
            user_id=user_id,
            project_id=project_id,
            original_text=text,
            translated_text=translated_text,
            source_language=source_lang,
            target_language=target_lang,
            context_used=True,
            context_summary=context_summary if context_summary else None,
            terms_detected=len(matched_terms)
        )
        db.add(translation)
        db.flush()  # ID ìƒì„±

        # íƒì§€ëœ ìš©ì–´ ì €ì¥
        for matched_term in matched_terms:
            if matched_term.glossary_term_id:
                term_mapping = TranslationTerm(
                    translation_id=translation.id,
                    glossary_term_id=UUID(matched_term.glossary_term_id),
                    position_start=matched_term.position_start,
                    position_end=matched_term.position_end,
                    matched_text=matched_term.matched_text
                )
                db.add(term_mapping)

        db.commit()
        db.refresh(translation)

        logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ ë° ì €ì¥: translation_id={translation.id}")

        # ì‘ë‹µ êµ¬ì„±
        return {
            "translation_id": str(translation.id),
            "original_text": text,
            "translated_text": translated_text,
            "source_language": source_lang,
            "target_language": target_lang,
            "context_used": True,
            "context_summary": context_summary,
            "detected_terms": [
                {
                    "matched_text": term.matched_text,
                    "position_start": term.position_start,
                    "position_end": term.position_end,
                    "glossary_term_id": term.glossary_term_id,
                    "korean_term": term.korean_term,
                    "english_term": term.english_term,
                    "vietnamese_term": term.vietnamese_term,
                    "definition": term.definition,
                    "domain": term.domain
                }
                for term in matched_terms
            ],
            "terms_count": len(matched_terms)
        }
