"""
Text processing utilities for AI agents.
Includes text chunking and term deduplication.
"""
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


def split_text_into_chunks(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 200
) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• .

    ì˜¤ë²„ë©ì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ ê²½ê³„ì—ì„œ ì •ë³´ ì†ì‹¤ì„ ë°©ì§€í•˜ê³ ,
    ë‹¨ì–´/ë¬¸ì¥ ê²½ê³„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸° (ì, ê¸°ë³¸ê°’: 1000)
        overlap: ì˜¤ë²„ë© í¬ê¸° (ì, ì¼ë°˜ì ìœ¼ë¡œ chunk_sizeì˜ 10-20%, ê¸°ë³¸ê°’: 200)

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸

    Example:
        >>> text = "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜" * 100  # 1400ì
        >>> chunks = split_text_into_chunks(text, chunk_size=1000, overlap=200)
        >>> len(chunks)  # 2ê°œ ì²­í¬ (0-1000, 800-1400)
        2
        >>> # ì˜¤ë²„ë© í™•ì¸
        >>> chunks[0][-200:] == chunks[1][:200]
        True

    Notes:
        - 1000ì â‰ˆ 250-330 í† í° (OpenAI ìµœëŒ€ 8191 í† í°)
        - ì˜¤ë²„ë©ìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´ ë° ê²€ìƒ‰ recall í–¥ìƒ
        - ë‹¨ì–´/ë¬¸ì¥ ê²½ê³„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
    """
    if not text or len(text) == 0:
        return []

    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end]

        # ë‹¨ì–´/ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ ë•Œë§Œ)
        if end < len(text):
            # ê³µë°±, ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì—ì„œ ìë¥´ê¸°
            last_boundary = max(
                chunk.rfind(' '),
                chunk.rfind('.'),
                chunk.rfind('\n'),
                chunk.rfind('ã€‚'),  # í•œê¸€ ë§ˆì¹¨í‘œ
                chunk.rfind('!')
            )

            # ê²½ê³„ê°€ ì²­í¬ì˜ 80% ì´ìƒ ìœ„ì¹˜ì— ìˆìœ¼ë©´ ì‚¬ìš©
            if last_boundary > chunk_size * 0.8:
                chunk = chunk[:last_boundary + 1]
                end = start + last_boundary + 1

        chunks.append(chunk.strip())

        # ë‹¤ìŒ ì‹œì‘ì : í˜„ì¬ ë - ì˜¤ë²„ë©
        start = end - overlap

        # ì˜¤ë²„ë© ë•Œë¬¸ì— ë¬´í•œë£¨í”„ ë°©ì§€
        if start >= len(text) - overlap:
            break

    avg_size = sum(len(c) for c in chunks) // len(chunks) if chunks else 0
    logger.info(f"ğŸ“Š Text split into {len(chunks)} chunks (size={chunk_size}, overlap={overlap}, avg={avg_size} chars)")
    return chunks


def deduplicate_terms(terms: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove duplicate terms based on korean term.
    If duplicates exist, keep the one with highest confidence.

    Args:
        terms: List of term dictionaries with 'korean' and 'confidence' keys

    Returns:
        List of unique terms

    Example:
        >>> terms = [
        ...     {"korean": "ìš©ì–´1", "confidence": 0.8},
        ...     {"korean": "ìš©ì–´1", "confidence": 0.9},  # duplicate with higher confidence
        ...     {"korean": "ìš©ì–´2", "confidence": 0.7}
        ... ]
        >>> unique = deduplicate_terms(terms)
        >>> len(unique)  # 2 (ìš©ì–´1 with 0.9, ìš©ì–´2 with 0.7)
        2
    """
    unique_terms = {}

    for term in terms:
        korean = term.get('korean', '').strip()
        if not korean:
            continue

        if korean not in unique_terms:
            unique_terms[korean] = term
        else:
            # Keep term with higher confidence
            existing_confidence = float(unique_terms[korean].get('confidence', 0))
            new_confidence = float(term.get('confidence', 0))
            if new_confidence > existing_confidence:
                unique_terms[korean] = term

    logger.info(f"ğŸ” Deduplicated: {len(terms)} â†’ {len(unique_terms)} unique terms")
    return list(unique_terms.values())
