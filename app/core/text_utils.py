"""
Text processing utilities for AI agents.
Includes text chunking, HTML stripping, and term deduplication.
"""
import logging
import re
from typing import List, Dict, Any
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


def strip_html_tags(html_text: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Outlook ë©”ì¼ì€ ë³´í†µ HTML í˜•ì‹ì´ë¯€ë¡œ, ì„ë² ë”©ê³¼ ê²€ìƒ‰ì— ì‚¬ìš©í•˜ê¸° ì „ì—
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ì—¬ ë²¡í„° í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

    Args:
        html_text: HTMLì´ í¬í•¨ëœ í…ìŠ¤íŠ¸

    Returns:
        HTML íƒœê·¸ê°€ ì œê±°ëœ ìˆœìˆ˜ í…ìŠ¤íŠ¸

    Example:
        >>> html = "<p>ì•ˆë…•í•˜ì„¸ìš”,</p><div>íšŒì‹ ì¥ì†ŒëŠ” <strong>ë“±ì´Œì¹¼êµ­ìˆ˜</strong>ì…ë‹ˆë‹¤.</div>"
        >>> clean = strip_html_tags(html)
        >>> print(clean)
        "ì•ˆë…•í•˜ì„¸ìš”,
        íšŒì‹ ì¥ì†ŒëŠ” ë“±ì´Œì¹¼êµ­ìˆ˜ì…ë‹ˆë‹¤."

    Notes:
        - BeautifulSoup4ë¡œ HTML íŒŒì‹± (ì•ˆì „í•˜ê³  ì •í™•í•¨)
        - ì—¬ëŸ¬ ê³µë°±/ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì •ë¦¬
        - ì•ë’¤ ê³µë°± ì œê±°
    """
    if not html_text or not html_text.strip():
        return ""

    try:
        # BeautifulSoupë¡œ HTML íŒŒì‹±
        soup = BeautifulSoup(html_text, 'html.parser')

        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        text = soup.get_text(separator='\n')

        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì œí•œ (ë‹¨ë½ êµ¬ë¶„ ìœ ì§€)
        text = re.sub(r'\n{3,}', '\n\n', text)

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r' {2,}', ' ', text)

        # ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(line for line in lines if line)

        return text.strip()

    except Exception as e:
        logger.warning(f"Failed to strip HTML tags: {str(e)}, returning original text")
        # HTML íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (fallback)
        return html_text


def split_text_into_chunks(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 200
) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• .

    ì˜¤ë²„ë©ì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ ê²½ê³„ì—ì„œ ì •ë³´ ì†ì‹¤ì„ ë°©ì§€í•˜ê³ ,
    ë‹¨ì–´/ë¬¸ì¥ ê²½ê³„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸° (ì, ê¸°ë³¸ê°’: 1000)
        overlap: ì˜¤ë²„ë© í¬ê¸° (ì, ì¼ë°˜ì ìœ¼ë¡œ chunk_sizeì˜ 10-20%, ê¸°ë³¸ê°’: 200)

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸

    Example:
        >>> text = "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜" * 100  # 1400ì
        >>> chunks = split_text_into_chunks(text, chunk_size=1000, overlap=200)
        >>> len(chunks)  # 2ê°œ ì²­í¬ (0-1000, 800-1400)
        2
        >>> # ì˜¤ë²„ë© í™•ì¸
        >>> chunks[0][-200:] == chunks[1][:200]
        True

    Notes:
        - 1000ì â‰ˆ 250-330 í† í° (OpenAI ìµœëŒ€ 8191 í† í°)
        - ì˜¤ë²„ë©ìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´ ë° ê²€ìƒ‰ recall í–¥ìƒ
        - ë‹¨ì–´/ë¬¸ì¥ ê²½ê³„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
    """
    if not text or len(text) == 0:
        return []

    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end]

        # ë‹¨ì–´/ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ ë•Œë§Œ)
        if end < len(text):
            # ê³µë°±, ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì—ì„œ ìë¥´ê¸°
            last_boundary = max(
                chunk.rfind(' '),
                chunk.rfind('.'),
                chunk.rfind('\n'),
                chunk.rfind('ã€‚'),  # í•œê¸€ ë§ˆì¹¨í‘œ
                chunk.rfind('!')
            )

            # ê²½ê³„ê°€ ì²­í¬ì˜ 80% ì´ìƒ ìœ„ì¹˜ì— ìˆìœ¼ë©´ ì‚¬ìš©
            if last_boundary > chunk_size * 0.8:
                chunk = chunk[:last_boundary + 1]
                end = start + last_boundary + 1

        chunks.append(chunk.strip())

        # ë‹¤ìŒ ì‹œì‘ì : í˜„ì¬ ë - ì˜¤ë²„ë©
        start = end - overlap

        # ì˜¤ë²„ë© ë•Œë¬¸ì— ë¬´í•œë£¨í”„ ë°©ì§€
        if start >= len(text) - overlap:
            break

    avg_size = sum(len(c) for c in chunks) // len(chunks) if chunks else 0
    logger.info(f"ğŸ“Š Text split into {len(chunks)} chunks (size={chunk_size}, overlap={overlap}, avg={avg_size} chars)")
    return chunks


def deduplicate_terms(terms: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove duplicate terms based on korean term.
    If duplicates exist, keep the one with highest confidence.

    Args:
        terms: List of term dictionaries with 'korean' and 'confidence' keys

    Returns:
        List of unique terms

    Example:
        >>> terms = [
        ...     {"korean": "ìš©ì–´1", "confidence": 0.8},
        ...     {"korean": "ìš©ì–´1", "confidence": 0.9},  # duplicate with higher confidence
        ...     {"korean": "ìš©ì–´2", "confidence": 0.7}
        ... ]
        >>> unique = deduplicate_terms(terms)
        >>> len(unique)  # 2 (ìš©ì–´1 with 0.9, ìš©ì–´2 with 0.7)
        2
    """
    unique_terms = {}

    for term in terms:
        korean = term.get('korean', '').strip()
        if not korean:
            continue

        if korean not in unique_terms:
            unique_terms[korean] = term
        else:
            # Keep term with higher confidence
            existing_confidence = float(unique_terms[korean].get('confidence', 0))
            new_confidence = float(term.get('confidence', 0))
            if new_confidence > existing_confidence:
                unique_terms[korean] = term

    logger.info(f"ğŸ” Deduplicated: {len(terms)} â†’ {len(unique_terms)} unique terms")
    return list(unique_terms.values())
